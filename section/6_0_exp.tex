\section{Evaluation of \Tool}
\label{sec:eva}

\input{section/tab_inhouse2}

Our experiments aim to evaluate whether \Tool 
can provide the desired profiling capability, accuracy and performance. 
Specifically, we answer the following three research questions:


\vspace*{3pt}
\noindent
 {\bf RQ1:} 
How effective is \Tool at identifying complexity problems
in both production and in-house environments?

\vspace*{3pt}
\noindent
{\bf RQ2:}
How accurate is the production-run version of \Tool after 
equipped with various instrumentation methods?


\vspace*{3pt}
\noindent
{\bf RQ3:}
Can the production-run version of \Tool be deployed
in production environment?



%\item {\bf RQ1:} 
%We apply sampling and other 
%approximation policies to design the production-run version of \Tool. 
%Will the production-run version still provide the same algorithmic 
%profiling results as the in-house version? 

%\item {\bf RQ2.} We design a ranking policy. 
%Will it be able to rank the root-cause function higher among all suspicious functions?

%\item {\bf RQ3.} What is the runtime overhead of the in-house and 
%production-run versions of \Tool. Can the the production-run version really be able to deployed in production runs?


\vspace*{2pt}


The first research question allows us to evaluate whether \Tool is effective 
at detecting real-world complexity problems. 
The second research question investigates whether the instrumentation methods 
used in the production-run version of \Tool can lower runtime overhead 
while remaining
the accuracy in detecting complexity problems. 
The third research question explores whether the overhead generated 
by the production-run version of \Tool is negligible 
enough for being used in production environment.





\subsection{Experimental Setup}

We implement \Tool using LLVM-5.0.0~\cite{llvm}. 
All our experiments are conducted on a Linux machine, 
with Intel Xeon(R) 4110 CPU and 3.10.0 kernel.
\Tool and all the data from the experiments are publicly 
available at https://github.com/ComAirProject/ComAir.

%We will release all our source code, experimental scripts 
%and used benchmarks in the future. 


%\commentty{Can we mention \Tool and datasets 
%are publicly  available?}

%\commentlh{addressed}

\subsubsection{Benchmarks}

We collect evaluated complexity problems from two sources. 
The first one is
from a public performance-bug benchmark 
suite~\cite{PerfBug,SongOOPSLA2014,ldoctor}, which is
studied in Section~\ref{sec:study}.
We used 17 bugs, and the others cannot be successfully
reproduced, because they depend on special hardware 
or software, which is not available to us. 
Among the 17 reproduced bugs, six are from
original Java or JS programs, 
and we re-implement them using C/C++.
The reason is that our current implementation of \Tool 
only supports C/C++ programs.
Our re-implementation is based on our thorough 
understanding of these complexity problems.
In our re-implementation, we maintain 
their original algorithms, 
data structures and caller-callee relationships. 
We also reproduced all three non-complexity, 
single-process, MySQL bugs used in~\cite{SongOOPSLA2014} 
to inspect whether \Tool will report 
any false positive when analyzing non-complexity bugs.

The second source is from Toddler~\cite{Alabama} and LDoctor~\cite{ldoctor}. 
Toddler is a dynamic technique that can detect inefficient loops in Java programs.
In total, developers confirmed 21 bugs detected by Toddler. 
These bugs were later re-implemented in C/C++ by LDoctor. 
Thus, we use all the 21 bugs from LDoctor in our evaluation.  


In total, we evaluate \Tool on 41 benchmark programs. 
All benchmark information is shown in Table~\ref{tab:benchmark_info}. 
Our evaluated original bugs are all large, and \commentlh{xxx} of them 
are more than 1 million lines of code.
Our evaluated re-implemented bugs are in small to middle sizes, 
ranging from 91 to 1094 lines of code. 
The majority of bugs used in our experiments are in $O(N^2)$ complexity. 
We also have two bugs in $O(2^N)$ complexity, four in O(N) complexity, 
and three non-complexity bugs.


\subsubsection{Production-run and In-house Inputs}

%\commentlh{
We create three sets of evaluation inputs to simulate three different scenarios.
Each set contains ten distinct inputs for every benchmark, 
following previous practice in evaluating statistical debugging~\cite{joy.asplos13,SongOOPSLA2014}.
The first scenario is the idea case. The sizes of the ten inputs 
constitute an arithmetic sequence with the largest input size equal 
to the input in the bug report
and the smallest input size equal to $1/10$ of the largest input. 
To obtain such inputs, we use bug reports, 
\commentty{better elaborate the sources of the bug reports}
which contain detailed information about how real-world 
users perceive the complexity problems. 
For {xxx} bugs, 
the users explicitly explained how to change input sizes 
to reproduce the described scaling problems in the bug reports. 
We follow the users' descriptions to create new inputs. 
For the other bugs, their bug-triggering inputs provided 
in the bug reports contain repetitive patterns, 
so that it is not difficult for us to create new inputs.
For example, when reporting GCC\#46401,
the user provided a C file, 
containing an expression with thousands of strings as operands. 
We change the number of strings to create new inputs.

The second scenario is in-house testing, where
developers can control the input size, 
but they do not use inputs with too large size 
to save testing time.  
The ten input sizes also constitute an arithmetic sequence, 
but the largest input size is only $1/100$ of the input size in the bug report. 

The last scenario is production environment, 
and we use input with random sizes. 
%}

\commentty{Are the three scenarios used in both in-house and production, or only the last scenario is
used in production? It'd be better to explain why certain inputs are suitable for in-house/production... it seems that
the first scenario can also be used in production because the inputs are from users.}

\subsubsection{Baseline Approaches}

...

\commentty{Can we put the baselines (e.g., statistical debugging, oprofile) here, and 
explain why we use them as baselines? The current baselines are hidden in the description,
which may make  careless reviewers feel that we do not have them at all. }


\subsubsection{Experimental Settings}

For each benchmark, we conduct algorithmic profiling on ten 
program runs with the ten distinct inputs for the two
versions of \Tool, following the practice in 
previous works~\cite{joy.asplos13,SongOOPSLA2014}.
After collecting a set of (input, cost) pairs for a code construct,
we only keep the pair with largest cost for pairs with the same input size,
since we want to infer the worst-case complexity.
We use $1/100$ as default sampling rate for 
the production-run version of \Tool, 
which is also the default sampling rate 
in previous works on statistical debugging~\cite{liblit03,CCI,ldoctor}.

\commentty{We never mention what sets of inputs are used
to answer the RQs.}

To answer RQ1 (i.e., the effectiveness of  \Tool in identifying
complexity problems), we measure the ranking number (position) of code
constructs containing complexity problems. 
To determine whether a code construct matches
the known complexity problem (i.e., ground truth), 
we manually examined the solution discussed 
in the corresponding issue 
report and the patch used for fixing the issue. 
To apply the production-run version of \Tool on each bug,
we first use a PIN tool~\cite{pin} to record executed iteration 
numbers for all loops using the bug-triggering input,
and then we instrument the top five loops with the largest loop 
iteration numbers to create five distinct program versions
deployed in production, 
since instrumenting all loops is expensive. 
For benchmarks with less than five executed 
loops during buggy runs, 
we instrument all executed loops. Selecting five loops follows the 
experimental setting in a previous work~\cite{ldoctor}.
For the in-house version, since the whole program is profiled, 
we rank all executed functions during buggy runs. 
\commentlh{
Since traditional profilers are also commonly used to diagnose performance problems,
we also evaluate oprofile using the largest input 
in each set to compare it with \Tool. 
}


To answer RQ2 (i.e., the accuracy of the production-run version of \Tool),
we calculate the $R^2$ value~\cite{rsquare} 
for the two groups of input sizes (and costs) reported by 
the production-run version of \Tool with and without 
instrumentation methods discussed in Section~\ref{sec:opt}.
$R^2$ represents how well one variable can be predicted by another variable,
and a $R^2$ value close to one means the two variables are 
in strong linear relationship~\cite{rsquare-value}.
Therefore, if the computed $R^2$ values for input sizes and costs
are both close to one,
the inferred two cost functions with and without instrumentation 
methods must be in the same order, 
which means that both sampling and approximation mechanisms 
do not hurt the accuracy. 



To answer RQ3 (i.e., whether the production-run version of \Tool 
can be really deployed in production runs), given a benchmark,
we measure the runtime overhead for the program version 
with the buggy loop instrumented, compared with non-instrumented version. 
We run each version \textbf{ten} times and compute the overhead based 
by average execution time. 



