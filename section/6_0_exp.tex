\section{Evaluation of \Tool}
\label{sec:eva}

\input{section/tab_inhouse2}

Our experiments aim to evaluate whether \Tool 
can provide the desired profiling capability, accuracy and performance. 
Specifically, we answer the following three research questions:


\vspace*{3pt}
\noindent
 {\bf RQ1:} 
How effective is \Tool at identifying complexity problems
in both production and in-house environments?

\vspace*{3pt}
\noindent
{\bf RQ2:}
How does the production-run version of \Tool 
compare with the in-house version in terms of
accuracy?

\vspace*{3pt}
\noindent
{\bf RQ3:}
Can the production-run version of \Tool be deployed
in production environment?



%\item {\bf RQ1:} 
%We apply sampling and other 
%approximation policies to design the production-run version of \Tool. 
%Will the production-run version still provide the same algorithmic 
%profiling results as the in-house version? 

%\item {\bf RQ2.} We design a ranking policy. 
%Will it be able to rank the root-cause function higher among all suspicious functions?

%\item {\bf RQ3.} What is the runtime overhead of the in-house and 
%production-run versions of \Tool. Can the the production-run version really be able to deployed in production runs?


\vspace*{2pt}


The first research question allows us to evaluate whether \Tool is effective 
at detecting real-world complexity problems. 
The second research question investigate whether the instrumentation methods 
used in the production-run version of \Tool incurs low cost while remaining
accuracy in detecting complexity problems. 
The third research question explores whether the overhead generated 
by \Tool is negligible enough for being used in production environment.



\subsection{Experimental Setup}

%\noindent{\textbf{Implementation and Experimental Platform.}}
We implement \Tool using LLVM-5.0.0~\cite{llvm}. 
All our experiments are conducted on a Linux machine, 
with Intel Xeon(R) 4110 CPU and 3.10.0 kernel.

%We will release all our source code, experimental scripts 
%and used benchmarks in the future. 


%\commentty{Can we mention \Tool and datasets 
%are publicly  available?}

%\commentlh{addressed}

\subsubsection{Benchmarks}

We collect evaluated complexity problems from two sources. 
The first one is
from a public performance-bug benchmark 
suite~\cite{PerfBug,SongOOPSLA2014,ldoctor}, which is
studied in Section~\ref{sec:study}.
We used 17 bugs, and the others cannot be successfully
reproduced, because they depend on special hardware 
or software, which is not available to us. 
Among the 17 reproduced bugs, six are from
original Java or JS programs, 
and we re-implement them using C/C++.
The reason is that our current implementation of \Tool 
only supports C/C++ programs.
Our re-implementation is based on our thorough 
understanding of these complexity problems.
In our re-implementation, we maintain 
their original algorithms, 
data structures and caller-callee relationships. 


The second source is from Toddler~\cite{Alabama} and LDoctor~\cite{ldoctor}. 
Toddler is a dynamic technique that can detect inefficient loops in Java programs.
In total, developers confirmed 21 bugs detected by Toddler. 
These bugs were later re-implemented in C/C++ by LDoctor. 
Thus, we use all the 21 bugs from LDoctor in our evaluation.  


In total, we evaluate \Tool on 38 benchmark programs. 
All benchmark information is shown in Table~\ref{tab:benchmark_info}. 
Our evaluated original bugs are all large, and seven of them 
are more than 1 million lines of code.
Our evaluated re-implemented bugs are in small to middle sizes, 
ranging from 91 to 1094 lines of code. 
The majority of bugs used in our experiments are in $O(N^2)$ complexity. 
We also have two bugs in $O(2^N)$ complexity 
and four in O(N) complexity.


\subsubsection{Production-run and In-house Inputs}

For the production-run version of \Tool, we create inputs used in our evaluation
by simulating real user inputs.
For each of the 38 benchmarks, we create ten distinct inputs, 
following previous practice in evaluating statistical debugging~\cite{joy.asplos13,SongOOPSLA2014}.
To obtain such inputs, we use bug reports, 
which contain detailed information about how real-world 
users perceive the complexity problems. 
For 14 bugs, 
the users explicitly explained how to change input sizes 
to reproduce the described scaling problems in the bug reports. 
We follow the users' descriptions to create new inputs. 
For the other bugs, their bug-triggering inputs provided 
in the bug reports contain repetitive patterns, 
so that it is not difficult for us to create new inputs.
For example, when reporting GCC\#46401,
the user provided a C file, 
containing an expression with thousands of strings as operands. 
We change the number of strings to create new inputs. 
Finally, we create ten distinct inputs for each bug, 
and we set their sizes 
to be an arithmetic sequence with the largest input size
equal to the input in the bug report and the 
smallest input size equal to $1/10$ 
of the largest input.
We use the same inputs to evaluate the in-house version of \Tool
to fairly compare accuracy and runtime 
overhead between the two versions of \Tool.



\subsubsection{Experimental Settings}

For each benchmark, we conduct algorithmic profiling on ten 
program runs with the ten distinct inputs for the two
versions of \Tool, following the practice in 
previous works~\cite{joy.asplos13,SongOOPSLA2014}.
After collecting a set of (input, cost) pairs for a code construct,
we only keep the pair with largest cost for pairs with the same input size,
since we want to infer the worst-case complexity.
We use $1/100$ as default sampling rate for 
the production-run version of \Tool, 
which is also the default sampling rate 
in previous works on statistical debugging~\cite{liblit03,CCI,ldoctor}.



To answer RQ1 (i.e., the effectiveness of  \Tool in identifying
complexity problems), we measure the rank number (position) of code
constructs containing complexity problems. We also use the rank number
to compute the percentage of code constructs
that need to be examined to find a performance bug in the program.
This ranking strategy has been widely adopted by existing fault localization 
techniques~\cite{Jones05,Cleve05}.
%\commentlh{
To evaluate the production-run version of \Tool,
we instrument the top five loops with the largest loop 
iteration numbers under bug-triggering inputs to 
create five distinct program versions, 
since instrumenting all loops is expensive. 
For benchmarks with less than five executed 
loops during buggy runs, 
we instrument all executed loops.  
For the in-house version, since the whole program is profiled, 
we rank all executed functions during buggy runs. 
%}

To answer RQ2 (i.e., the accuracy of the production-run version of \Tool),
we calculate the $R^2$ value~\cite{rsquare} 
for the two groups of input sizes (and costs) reported by 
the two versions of \Tool.
$R^2$ represents how well one variable can be predicted by another variable,
and a $R^2$ value close to one means the two variables are 
in strong linear relationship~\cite{rsquare-value}.
Therefore, if the computed $R^2$ values for input sizes and costs
are both close to one,
the inferred two cost functions by the two versions must be in the same order, 
which means the production-run version is as accurate as the in-house version, 
and both sampling and other designed approximation (Section~\ref{sec:opt}) do not hurt accuracy. 


Due to the difference in profiling granularities and algorithms 
(e.g. whether different dynamic instances in one program run are merged), 
given a bug in $O(N)$ complexity, 
we apply the production-run version on the buggy loop, 
and apply the in-house version on the function containing the buggy loop. 
Given a bug in $O(N^2)$ complexity, we apply the production-run version on the inner loop, 
while applying the in-house version on the function containing the outer loop. 
The two bugs in $O(N^2)$ complexity are both caused by recursive functions.
We apply the two versions on the recursive function for each bug. 
To understand whether sampling
causes the production-run version of \Tool 
to lose some accuracy, we also calculate the $R^2$ value for two groups 
of inputs (and costs) reported by the production-run versions 
under sampling rate $1/100$ and sampling rate $1$,
since $1/100$ is our default sampling rate and $1$ means disable sampling. 
\commentty{This paragraph seems too much. Are we just comparing 
the accuracy between the non-optimized and the optimized production version of \Tool}


%\commentlh{
To answer RQ3 (i.e., whether the production-run version of \Tool 
can be really deployed in production runs), 
we measure the runtime overhead for the program version 
with the buggy loop instrumented, compared with non-instrumented version. 
We run each version ten times and compute the overhead based by average execution time. 



