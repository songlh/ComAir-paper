\section{Evaluation of \Tool}
\label{sec:eva}

\input{section/tab_inhouse}

Our experiments aim to evaluate whether \Tool 
can provide the desired profiling capability, accuracy and performance. 
Specifically, we answer the following three research questions:


\vspace*{3pt}
\noindent
 {\bf RQ1:} 
How effective is \Tool at identifying complexity problems
in both production and in-house environments?

\vspace*{3pt}
\noindent
{\bf RQ2:}
How does the production-run version of \Tool 
compare with the in-house version in terms of
%cost-effectiveness?
\commentlh{accuracy}?

\vspace*{3pt}
\noindent
{\bf RQ3:}
Can the production-run version of \Tool be deployed
in production environment?



%\item {\bf RQ1:} 
%We apply sampling and other 
%approximation policies to design the production-run version of \Tool. 
%Will the production-run version still provide the same algorithmic 
%profiling results as the in-house version? 

%\item {\bf RQ2.} We design a ranking policy. 
%Will it be able to rank the root-cause function higher among all suspicious functions?

%\item {\bf RQ3.} What is the runtime overhead of the in-house and 
%production-run versions of \Tool. Can the the production-run version really be able to deployed in production runs?


\vspace*{2pt}


The first research question allows us to evaluate whether \Tool is effective 
at detecting real-world complexity problems. 
The second research question investigate whether the instrumentation methods 
used in the production-run version of \Tool incurs low cost while remaining
accuracy in detecting complexity problems. 
The third research question explores whether the overhead generated 
by \Tool is negligible enough for being used in production environment.



\subsection{Experimental Setup}

%\noindent{\textbf{Implementation and Experimental Platform.}}
We implement \Tool using LLVM-5.0.0~\cite{llvm}. 
All our experiments are conducted on a Linux machine, 
with Intel Xeon(R) 4110 CPU and 3.10.0 kernel.

%We will release all our source code, experimental scripts 
%and used benchmarks in the future. 


%\commentty{Can we mention \Tool and datasets 
%are publicly  available?}

%\commentlh{addressed}

\subsubsection{Benchmarks}

We collect evaluated complexity problems from two sources. 
The first one is
from a public performance-bug benchmark 
suite~\cite{PerfBug,SongOOPSLA2014,ldoctor}, which is
studied in Section~\ref{sec:study}.
We used 17 bugs, and the others cannot be successfully
reproduced, because they depend on special hardware 
or software, which is not available to us. 
Among the 17 reproduced bugs, six are from
original Java or JS programs, 
and we re-implement them using C/C++.
The reason is that our current implementation of \Tool 
only supports on C/C++ programs.
Our re-implementation is based on our thorough 
understanding of these complexity problems.
In our re-implementation, we maintain 
their original algorithms, 
data structures and caller-callee relationships. 


The second source is from Toddler~\cite{Alabama} and LDoctor~\cite{ldoctor}. 
Toddler is a dynamic technique that can detect inefficient loops in Java programs.
In total, developers confirmed 21 bugs detected by Toddler. 
These bugs were later re-implemented in C/C++ by LDoctor. 
Thus, we use all the 21 bugs from LDoctor in our evaluation.  


In total, we evaluate \Tool on 38 benchmark programs. 
All benchmark information is shown in Table~\ref{tab:benchmark_info}. 
Our evaluated original bugs are all large, and seven of them 
are more than 1 million lines of code.
Our evaluated re-implemented bugs are in small to middle sizes, 
ranging from 91 to 1094 lines of code. 
The majority of bugs used in our experiments are in $O(N^2)$ complexity. 
We also have two bugs in $O(2^N)$ complexity, 
and two bugs in O(N) complexity.


\subsubsection{Production-run and In-house Inputs}

For the production-run version of \Tool, we create 10 distinct inputs
for each of the 38 benchmarks by simulating real user inputs. 
\commentty{people may ask why 10?}
To obtain such inputs, we use bug reports, 
which contain detailed information about how real-world 
users perceive the complexity problems. 
For 14 bugs, 
the users explicitly explained how to change input sizes 
to reproduce the described scaling problems in the bug reports. 
We follow the users' descriptions to create new inputs. 
For the other bugs, their bug-triggering inputs provided 
in the bug reports contain repetitive patterns, 
so that it is not difficult for us to create new inputs.
For example, when reporting GCC\#46401,
the user provided a C file, 
containing an expression with thousands of strings as operands. 
We change the number of strings to create new inputs. 
Finally, we create ten distinct inputs for each bug, 
and we set their sizes 
to be an arithmetic sequence with the largest input size
equal to the input in the bug report and the 
smallest input size equal to $1/10$ 
of the largest input.
We use the same inputs to evaluate the in-house version of \Tool. 
\commentty{Is it because by using the same inputs, we can compare
the overheads between the two versions?}





\subsubsection{Experimental Settings}

For each benchmark, we conduct algorithmic profiling on ten 
program runs with the ten distinct inputs for both production-run 
and in-house versions of \Tool, following the practice in 
previous work~\cite{joy.asplos13,SongOOPSLA2014}.
After collecting a set of (input, cost) pairs for a code construct,
we only keep the pair with largest cost for pairs with the same input size,
since we want to infer the worst-case complexity.
We use $1/100$ as default sampling rate for the production-run version of \Tool.


To answer RQ1 (i.e., the effectiveness of  \Tool in identifying
complexity problems), we measure the rank number (position) of code
constructs containing complexity problems. We also use the rank number
to compute the percentage of code constructs
that need to be examined to find a performance bug in the program.
This ranking strategy has been widely adopted by existing fault localization 
techniques~\cite{Jones05,Cleve05}.
%\commentlh{
To evaluate the production-run version of \Tool,
we instrument the top five loops with largest loop 
iteration numbers under bug-triggering input to create five distinct program versions, 
since instrumenting all loops is expensive. 
For benchmarks with less than five executed 
loops during buggy runs, 
we instrument all executed loops.  
For the in-house version, since the whole program is profiled, 
we rank all executed functions during buggy runs. 
%}

%\commentlh{
To answer RQ2 (i.e., the accuracy of the production-run version of \Tool, 
comparing with the in-house version),
we calculate the $R^2$ value for the two groups of input sizes (and cost) reported by 
the production-run version
and the in-house version respectively.
\commentty{Need to explain what $R^2$ is and how it is computed.}
If the $R^2$ value is close to one, the two groups of inputs (or costs) 
are in linear relationship and they will not change 
the order of the inferred cost function. 
\commentty{It is not clear here why $R^2$ can reflect the comparison
of the accuracy between the two versions.}
Given an evaluated benchmark, we conduct this experiment 
on the minimum
code construct containing the complexity problem.
\commentty{what do you mean by ``minimum code construct"?}
To understand whether the optimization methods
(Section ???) cause the production-run version of \Tool 
to lose some accuracy, we also calculate the $R^2$ value for two groups 
of inputs (and costs) reported by the production-run versions 
under sampling rate 100 and sampling rate 1. 
\commentty{people may ask why 100 and 1 are chosen.}
%}

%\commentlh{
To answer RQ3 (i.e., whether the production-run version of \Tool 
can be really deployed in production runs), 
we measure the runtime overhead for the program version 
with the loop containing complexity problems instrumented. 
\commentty{not sure why this can be used to measure if the production-version
can be deployed...I thought we should measure the overhead of the instrumented version
vs the non-instrumented version?}
As a contrast, we also measure the overhead for the in-house version of \Tool, 
and the minimum of instrumented programs in order 
to identify the complexity problem.  \commentty{what do we need to
care about the in-house version for this RQ?}
%}


\subsection{Limitations and Threats to Validity}

\commentty{Consider adding a section
on limitations and threats to validity after the results. People would want to see under what conditions
\Tool works and does not work.}

The primary threat to external validity 
for this study involves the representativeness 
of our benchmark programs and inputs used
to compute cost functions.
Other benchmarks and inputs may exhibit different 
behaviors and cost-benefit tradeoffs.
However, we do reduce this threat to some extent by
using several varieties of well studied open source subjects 
for our study, and inputs generated by practical approaches.

The primary threat to internal validity for this study
is possible faults in the implementation of our approach
and in the tools that we use to perform evaluation.
We controlled for this threat by extensively testing
our tools and verifying their results against a smaller
program for which we can manually determine the correct results.

Where construct validity is concerned, ...

