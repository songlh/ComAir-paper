\section{Evaluation of \Tool}
\label{sec:eva}

\input{section/tab_inhouse}

Our experiments aim to evaluate whether \Tool 
can provide the desired profiling capability, accuracy and performance. 
Specifically, we answer the following three research questions:


\vspace*{3pt}
\noindent
 {\bf RQ1:} 
How effective is \Tool at identifying complexity problems
in both production and in-house environments?

\vspace*{3pt}
\noindent
{\bf RQ2:}
How does the production-run version of \Tool 
compare with the in-house version in terms of
%cost-effectiveness?
\commentlh{accuracy}?

\vspace*{3pt}
\noindent
{\bf RQ3:}
Can the production-run version of \Tool be deployed
in production environment?



%\item {\bf RQ1:} 
%We apply sampling and other 
%approximation policies to design the production-run version of \Tool. 
%Will the production-run version still provide the same algorithmic 
%profiling results as the in-house version? 

%\item {\bf RQ2.} We design a ranking policy. 
%Will it be able to rank the root-cause function higher among all suspicious functions?

%\item {\bf RQ3.} What is the runtime overhead of the in-house and 
%production-run versions of \Tool. Can the the production-run version really be able to deployed in production runs?


\vspace*{2pt}


The first research question allows us to evaluate whether \Tool is effective 
at detecting real-world complexity problems. 
The second research question investigate whether the instrumentation methods 
used in the production-run version of \Tool incurs low cost while remaining
accuracy in detecting complexity problems. 
The third research question explores whether the overhead generated 
by \Tool is negligible enough for being used in production environment.



\subsection{Experimental Setup}

%\noindent{\textbf{Implementation and Experimental Platform.}}
We implement \Tool using LLVM-5.0.0~\cite{llvm}. 
All our experiments are conducted on a Linux machine, 
with Intel Xeon(R) 4110 CPU and 3.10.0 kernel.

We will release all our source code, experimental scripts 
and used benchmarks in the future. 


\commentty{Can we mention \Tool and datasets 
are publicly  available?}

\commentlh{addressed}

\subsubsection{Benchmarks}

We collect evaluated complexity problems from two sources. 
The first one is
from a public performance-bug benchmark 
suite~\cite{PerfBug,SongOOPSLA2014,ldoctor}, which is
studied in Section~\ref{sec:study}.
We used 17 bugs, and the others cannot be successfully
reproduced, because they depend on special hardware 
or software, which is not available to us. 
Among the 17 reproduced bugs, six are from
original Java or JS programs, 
and we re-implement them using C/C++.
The reason is that our current implementation of \Tool 
only supports on C/C++ programs.
Our re-implementation is based on our thorough 
understanding of these complexity problems.
In our re-implementation, we maintain 
their original algorithms, 
data structures and caller-callee relationships. 


The second source is from Toddler~\cite{Alabama} and LDoctor~\cite{ldoctor}. 
Toddler is a dynamic technique that can detect inefficient loops in Java programs.
In total, developers confirmed 21 bugs detected by Toddler. 
These bugs were later re-implemented in C/C++ by LDoctor. 
Thus, we use all the 21 bugs from LDoctor in our evaluation.  


In total, we evaluate \Tool on 38 benchmark programs. 
All benchmark information is shown in Table~\ref{tab:benchmark_info}. 
Our evaluated original bugs are all large, and seven of them 
are more than 1 million lines of code.
Our evaluated re-implemented bugs are in small to middle sizes, 
ranging from 91 to 1094 lines of code. 
The majority of bugs used in our experiments are in $O(N^2)$ complexity. 
We also have two bugs in $O(2^N)$ complexity, 
and two bugs in O(N) complexity.


\subsubsection{Production-run and In-house Inputs}

\commentty{We'd better clarify what  inputs
were used and how they were generated. SE people 
are very picky about this kind of thing.}

\commentty{how to obtain the inputs?}

\commentty{How were inputs generated for the in-house version?}

\commentlh{addressed, but not good enough}


For the production-run version of \Tool, the inputs
are from real users. 
To obtain suitable inputs for our evaluation, 
we mainly leverage bug reports, 
which contain detailed information about how real-world 
users perceive the complexity problems. 
For 14 bugs, 
the users explicitly explained how to change input sizes 
to reproduce the described scaling problems in the bug reports. 
We follow the users' descriptions to create new inputs. 
For the other bugs, their bug-triggering inputs provided 
in the bug reports contain repetitive patterns, 
so that it is not difficult for us to create new inputs.
For example, when reporting GCC\#46401,
the user provided a C file, 
containing an expression with thousands of strings as operands. 
We change the number of strings to create new inputs. 
We create ten distinct inputs for each bug, 
and we set their sizes 
to be an arithmetic sequence with the largest input size
equal to the input in the bug report and the 
smallest input size equal to $1/10$ 
of the largest input.
We use the same inputs to evaluate the in-house version of \Tool. 






\subsubsection{Experimental Settings}

For each benchmark, we conduct algorithmic profiling on ten 
program runs with the ten distinct inputs for both production-run 
and in-house versions of \Tool, following the practice in 
previous work~\cite{joy.asplos13,SongOOPSLA2014}.
After collecting a set of (input, cost) pairs for a code construct,
we only keep the pair with largest cost for pairs with the same input size,
since we want to infer the worst-case complexity.
We use $1/100$ as default sampling rate for the production-run version of \Tool.


To answer RQ1 (i.e., the effectiveness of  \Tool in identifying
complexity problems), we measure the rank number (position) of code
constructs contain complexity problems. We also use the rank number
to compute the percentage of code constructs
that need to be examined to find a performance bug in the program.
This ranking strategy has been widely adopted by existing fault localization 
techniques~\cite{Jones05,Cleve05}.
\commentlh{
To evaluate the production-run version of \Tool,
we instrument the top five loops with largest loop 
iteration numbers under the bug-triggering input 
to create five distinct program versions, 
since instrumenting all loops is too time-consuming. 
For benchmarks with less than five executed 
loops during buggy runs, 
we instrument all executed loops.  
For the in-house version, since the whole program is profiled, 
we rank all executed functions during buggy runs.}


\commentlh{
To answer RQ2 (i.e., the accuracy of the production-run version of \Tool),
besides reporting inferred cost functions,
we also compute $R^2$ value~\cite{rsquare} for 
the two groups of input sizes (and cost values) reported by 
the production-run version
and the in-house version.
If $R^2$ value is close to one~\cite{rsquare-value}, 
the two groups of input sizes (or cost values) 
are in strong linear relationship and 
replacing one with the other will not change 
the order of an inferred cost function. 
For each evaluated benchmark, 
we choose the two groups of input sizes (and cost) reported
when profiling the minimum code constructs that 
the production-run version 
and the in-house version can report the corresponding complexity problem. 
To better understand how sampling influences the accuracy,
we also compute $R^2$ value for two groups 
of input sizes (and cost values) reported by the production-run version 
of \Tool configured 
with sampling rate as $1/100$ and $1$. 
}

\commentlh{
To answer RQ3 (i.e., whether the runtime overhead of \Tool), 
we measure the runtime overhead when the loop containing the complexity problem 
is instrumented. 
As a contrast, we also measure the overhead for the in-house version of \Tool.
Since the in-house version of \Tool profiles the whole program, 
to compare the overhead more fairly, we also measure 
the runtime overhead for the in-house version of \Tool with 
the minimum code to be instrumented in order to identify 
the corresponding complexity problem. 

}



\subsection{Limitations and Threats to Validity}

\commentty{Consider adding a section
on limitations and threats to validity after the results. People would want to see under what conditions
\Tool works and does not work.}

The primary threat to external validity 
for this study involves the representativeness 
of our benchmark programs and inputs used
to compute cost functions.
Other benchmarks and inputs may exhibit different 
behaviors and cost-benefit tradeoffs.
However, we do reduce this threat to some extent by
using several varieties of well studied open source subjects 
for our study, and inputs generated by practical approaches.

The primary threat to internal validity for this study
is possible faults in the implementation of our approach
and in the tools that we use to perform evaluation.
We controlled for this threat by extensively testing
our tools and verifying their results against a smaller
program for which we can manually determine the correct results.

Where construct validity is concerned, ...

