%\newpage
\section{\Tool Design}
\label{sec:online}


{\color{red} Todo: change the design principle}
In this section, we discuss our design and 
implementation for the production-run version of \Tool. 
For production-run usage, profiles are collected from the user side.
It is very important to keep the runtime overhead low, since
end users cannot tolerate any observable performance slowdown.
To achieve this requirement,
our design follows several principles. 

First, \textit{study guided}. 
The design of the production-run version of \Tool
is guided by the empirical study in Section~\ref{sec:study}.
We focus on the majority of complexity problems, 
caused either by repeated executions of a loop ($N^k$)
or a recursive function ($2^N$).
We focus on common types of data structures, which are array and linked list.

Second, \textit{focused checking}.
When applying the production-run version of \Tool, 
we expect developers will specify a suspicious loop or a suspicious recursive function
to be monitored. 
\Tool will automatically instrument the specified code construct 
to collect runtime information from the user side in a low overhead. 
The profiling results can help developers better understand the complexity of the code construct 
and its processed workloads in the real world.
\Tool can be used together with performance failure 
diagnosis tools\cite{SongOOPSLA2014} 
or traditional profilers to
focus on suspicious code constructs leading 
to user-perceived performance failures.

Third, \textit{sampling}.
Instead of recording all dynamic information, 
we apply sampling and record only part of the information. 
We infer information for the whole execution based on the collected samples. 
Sampling can effectively lower the runtime overhead. 


\subsection{Technical Design}
As we discussed in Section~\ref{sec:study}, 
the majority of complexity problems studied are caused 
by repeated executions of a loop or a recursive function. 
Previous works show that sampling code constructs that are executed 
multiple times in one program run can lower the runtime overhead, 
while still being able to collect enough runtime information 
without hurting diagnosis latency\cite{SongOOPSLA2014,ldoctor}. 
Inspired by our study and the earlier works, 
we apply sampling to efficiently profile loops 
and recursive functions with multiple executions. 

\noindent\textbf{Input Metric}
If the specified code construct is an array-processing loop 
or a linked-list-processing loop,
we will use DSS as the input metric. 
Otherwise, we will use RMS+ as the input metric. 

As we discussed in Section~\ref{sec:inhouse}, 
there are two methods, top-down and bottom-up, 
to analyze RMS+ and DSS records collected 
for multiple dynamic instances of a code construct in one program run. 
We leverage the top-down method for the in-house version of \Tool. 
However, the top-down method is not suitable for sampling. 
The reason is as follows.
Assume we have a code construct \texttt{c} to monitor. 
It is inside a loop \texttt{l} and is executed multiple times in one program run.
There are fewer dynamic instances of \texttt{l}
than dynamic instances of \texttt{c}.
If we apply the top-down method, 
we need to sample dynamic instances of \texttt{l}, 
and it is very likely that we will miss these instances. 
For each dynamic instance of \texttt{l}, 
there will be more computation, 
compared with an instance of \texttt{c}.
More overhead will be incurred to collect 
information for dynamic instances of \texttt{l}.
Therefore, we apply the bottom-up method 
in the production-run version of \Tool.
We sample instances of \texttt{c} to record 
distinct memory cells contributing RMS+ 
or distinct elements in an array or a linked list.
We use the sampled information to infer RMS+ 
or DSS for all instances of \texttt{c} in one program run.


The sampling method is similar to that described in previous works on statistical 
debugging\cite{liblit03,liblit05,CCI,SongOOPSLA2014,ldoctor}.
We make a cloned version of the monitored code construct.
We instrument the cloned version to record information for RMS+ or DSS. 
We dump the recorded information to log 
whenever the cloned version finishes execution. 
We add extra delimiters to log to differentiate information collected from different instances.
Before each execution of the monitored code construct, 
we choose between the cloned version and the original version. 
How many times the cloned version is executed 
depends on a tunable sampling rate. 
To make the choice between the two versions,
we add a global counter to the monitored program. 
If the counter value is larger than $0$, 
we choose the original version and decrease the counter value by $1$.
If the counter value is equal to $0$,
we choose the cloned version and reset the counter value to 
a random number, 
whose expectation is equal to the inverse of the sampling rate.  


We leverage the mark-and-recapture method\citep{mark-recapture} to 
estimate RMS+ or DSS for all dynamic instances of a code construct 
based on the collected samples. 
Mark-and-recapture is a commonly used statistical method 
for estimating the size of an animal population. 
In this method, some of the animals are captured, marked, and released. 
Then, another group of the animials are captured.
The size of the whole animal population is estimated 
based on the ratio of marked animals in the second captured sample.  


Each sample of a monitored code construct is a set of memory cells, 
which contribute RMS+ or represent distinct elements in an array or a linked list. 
In one program run, we assume that we collect a sequence of $m$ samples. 
Given the $i$th sample, $M_i$ represents the 
total number of distinct memory cells in the previous $i-1$ samples, 
$C_i$ represents the number of distinct memory cells in the $i$th sample,
and $R_i$ represents the number of distinct memory cells in 
the $i$th sample that also appeared in one of the previous $i-1$ samples.
The total number of distinct memory cells for all dynamic instances 
of the monitored code construct can be estimated as:


%\begin{equation} \label{eq:mark}
%$$p_v(S_v(a)) = 1 - \prod\limits_{u \in S_v(a)}(1 - p_{u,v})$$
%N = \frac{\sum\limits_{i=1}^m M_i*C_i}{\sum\limits_{i=1}^m R_i}
%\end{equation}

\begin{equation} \label{eq:mark}
N = \sum\limits_{i=1}^m M_i*C_i\Big/\sum\limits_{i=1}^m R_i
\end{equation}

\noindent\textbf{Cost Metric}
The production-run version of \Tool focuses on recursive functions or loops.
If the monitored code construct is a recursive function,
we use RIs as the cost metric.
If the monitored code construct is a loop, 
we use LIs as the cost metric. 
We do not apply sampling when collecting these two metrics. 
As we discussed in Section~\ref{sec:inhouse},
these two metrics will incur a smaller overhead compared with BBs, 
and they can still provide accurate profiling results.  


\subsection{Implementation}

{\color{red} Todo: How to implement RMS and DSS}

\subsection{Static Optimizations}

\noindent{\color{red} Todo: How to efficiently count BB}

\noindent{\color{red} Todo: The optimization for array}

\noindent{\color{red} Todo: more loop-invariant address outside of the loop}

\noindent{\color{red} Todo: remove unnecessary read/write instrumentation}









