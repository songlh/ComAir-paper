\section{\Tool Design and Implementation}
\label{sec:online}

%In this section, we present technical details 
%for the production-run version of \Tool, 
%including the high-level design choices
%and several optimization policies. 

Guided by the findings of empirical study in Section~\ref{sec:back},
we design and implement \Tool, a performance \commentlh{algorithmic?}
profiler to detect complexity problems. \Tool
can be used in both production environment 
and in-house development. We first describe
the production-run version of \Tool. 
The key benefits of \Tool include its \emph{low runtime} 
overhead and capability of \emph{ranking 
and localizing complexity problems in specific code constructs}. 
We then describe how to use \Tool as an in-house testing tool. 

The design of \Tool used in production environment 
follows two steps. First, given a program under profiling,
\Tool instruments the program and
distribute it to end users to collect runtime profiles
under different user inputs (e.g., workload). 
To minimize runtime overhead, we design and implement
several advanced instrumentation methods in \Tool. 
In the second step, \Tool computes cost functions for
different code constructs based on the execution profiles
and then reports a ranked list of code constructs in
terms of their likelihood of containing performance bugs due to
complexity problems.  We next describe details of the two steps. 


\subsection{Using Light-Weighted Instrumentation 
for Algorithmic Profling}

To obtain execution profiles, we need to select suitable 
\emph{profiling targets} (i.e.,
code constructs), \emph{input metrics}, and \emph{cost metrics}. 
\commentlh{
\Tool supports code constructs in any granularity. 
However, one finding in our empirical study is 
that all complexity problems are 
caused either by a loop or a recursive function. 
Therefore, we recommend code constructs that related 
to loops and recursive functions as profiling targets. 
Distinct code constructs are instrumented by the production-run
version of \Tool to create different program versions.}
As for input metrics, our study found that most two commonly involved 
data structures in complexity problems 
are array and linked list. Therefore, if a loop is chosen as a profiling target 
and it is to process an array 
or a linked list, we use data structure size (DSS) as its input metric. 
Otherwise, we use Read Memory Size (RMS), 
since RMS is more generic as we discussed in Section~\ref{sec:back}. 
The difference between DSS and RMS is that 
we only need to instrument \texttt{read} instructions accessing elements 
in an array or a linked list for DSS,  without instrument \texttt{write} instructions.
Therefore, DSS requires to record less dynamic information. 
The cost metric used in \Tool is executed basic 
blocks (BBs), 
\commentlh{
and we have two approximating metrics 
for loops and recursive functions respectively.  
}


As discussed in Section~\ref{subsec:existing}, 
existing algorithmic profiling techniques
often incur significant overhead and thus cannot be used in production
environment. To address this issue, \Tool proposes several 
methods to enable practical online instrumentation, including 1) 
minimizing instrumented code at each user's end; 
2) optimizing instrumentation sites;
3) approximation; and 4) sampling. 



\subsubsection{Creating Multiple Instrumentation Versions}

\Tool creates multiple versions of program under profiling, 
and for each version \Tool only instruments a small number of 
profiling targets. 
All different versions are distributed to different end users. 
Since there is a huge amount of end users, 
we anticipate that enough profiles can still be collected 
for each version to conduct algorithmic profiling. 

To select profiling targets, \commentty{How to determine
which code targets to instrument in each version?}
\commentlh{we are largely guided by our empirical 
study in Section~\ref{sec:tax}.
Since all complexity problems involve a 
loop or a recursive function, 
we choose loops, recursive functions, functions containing loops,
and functions invoked inside a loop directly 
or indirectly as profiling targets. 
We ignore loops whose iteration numbers are constant,
since the execution of these loops does not scale with input 
and cannot cause complexity problems.} 





\subsubsection{Optimizing Instrumentation Sites}

%We briefly overview the algorithm 
%designed in previous works~\cite{Aprof1,Aprof2} 
%before discussing our two designed optimizations. 

To measure RMS (i.e., the input metric), we need to instrument 
and the \texttt{read} and \texttt{write} instructions in every 
code construct (or profiling target). \commentty{not sure if I'm correct.}
\commentlh{correct.}
\commentlh{
Different from the algorithm described 
in previous works~\cite{Aprof1,Aprof2},
we do not maintain any global data structure,
since we want to simplify online monitoring functionalities 
and reduce runtime overhead.
Instead, we trace memory addresses 
for \texttt{read} and \texttt{write} instructions 
and we analyze tracing log offline to calculate RMS. 
}

In order to reduce the number of \texttt{read} 
and \texttt{write} instructions
to be instrumented, our approach works as follows. 
If a write on a memory cell happens earlier, 
following read on the same memory cell will not increase RMS,
\commentlh{since RMS only count memory cells whose first access is read.
Similarly, if a read on a memory cell happens earlier,
following read on the same memory cell will not increase RMS either,
since RMS only considers distinct memory cells. 
Therefore, given two consecutive memory accesses on the same location,
we do not need to instrument the second one. 
We design an intra-procedural analysis, based on dominator analysis,
which can tell us when an instruction is executed 
whether another instruction has to be executed in advance.}
We focus on stack memory cells that hold scalar variables 
and only have read and write as uses 
(i.e., not having ``address of'' as uses), 
so that alias analysis is avoided. 


\commentlh{
When counting the executed basic blocks (BBs) to compute cost metric,
a naive way is to instrument every basic block to increment a counter by one.
To reduce the number of instrumentation 
sites that update the counter,
we propose a method to selectively 
instrument part of basic blocks.}
\commentty{optimization in what way?} 
we employ an algorithm \commentty{does the algorithm
have a name?}, \commentlh{no}
which was originally designed to efficiently 
count edge events through selectively instrumenting counter update sites 
on control flow graphs (CFGs)~\cite{event-counting}.
The algorithm has already been proved to conduct path 
profiling efficiently~\cite{path-profiling,peter-ase}.  
Since the original algorithm is design to count edge events, 
we split each BB into two and label the event number to be one for 
edges connecting a pair of split BBs, 
leaving the event number as zero for other edges. 
After that, applying the algorithm~\cite{event-counting}
can tell us optimized instrumentation \commentlh{policy 
about where to update the couter and how to update the counter.} 


\subsubsection{Approximation of Input and Cost Metrics}

\commentty{Approximation of what? I think we should
make the title more complete.}

Accurately recording dynamic information for input and cost metrics
for each code construct
may incur a large runtime overhead.
Therefore, we propose two approximation mechanisms. 
\commentlh{
First, if a profiled code construct is a loop and 
it is to process an array or a linked list,
we use the DSS as input metric.
Second, if a profiled code construct is a loop or a recursive function,
we use loop iterations (LIs) and recursive invocations (RIs) 
as cost metrics respectively. 
}

To apply the first approximation, we are facing two challenges. 
The first one is to \emph{identify array-processing loops}.
To address this, given a loop, we analyze all pointer usage inside the loop. 
If a pointer is deferenced in every iteration of a loop, 
and its value is also increased or decreased by 
an integer number in every iteration,
we consider the pointer points to array elements and 
the loop is an array-processing loop.  
For example, as shown in Figure~\ref{fig:mysql27287}, 
\texttt{p} points to array elements. 
It is deferenced in every iteration of the loop 
in Figure~\ref{fig:mysql27287},
and the pointer value is decreased by one in every loop iteration. 
\commentlh{
For an array-processing loop, 
instead of recording all addresses of 
accessed array elements,
we only record the addresses 
of the first and the last accessed elements. 
We calculate the address difference and use it as DSS.
The reason is that array is organized as a 
consecutive memory, and the address difference 
between the first and last accessed elements is roughly in linear 
relationship with the number of accessed elements and DSS. 
}


\commentty{Why do we need to handle linked-list-processing loops?
In the first paragraph, only array-processing loop is mentioned.}
\commentlh{I have changed the first paragraph in this subsection.}
The second challenge is to \emph{identify linked-list-processing loops}.
%\noindent{\underline{How to identify a linked-list-processing loop?}}
To address this, we also analyze all pointer usage inside a loop under profile, 
by checking whether a pointer satisfies the following three conditions.
First, its base type is a \texttt{struct}.
Second, it is deferenced in every iteration of the loop.
Third, it is updated in every iteration with a new value 
from one field of the \texttt{struct} it points to.
If a pointer satisfies the three conditions, 
we consider it points to elements in a linked list.
We also consider the loop is a linked-list-processing loop. 



\subsubsection{Sampling Dynamic Instances of Profiling Targets}

\commentty{Again, sampling what?}

Previous work~\cite{liblit03,liblit05,CCI,SongOOPSLA2014,ldoctor}
demonstrate that sampling can greatly 
reduce overhead for dynamic techniques,
while still preserving their desired 
detection and diagnosis capability. 
This motivates us to apply sampling to algorithmic profiling. 
Our study in Section~\ref{sec:back} shows that 
the majority of complexity problems are caused by repeated execution of a loop
or a recursive function. 
\commentlh{
This result inspires us to sample some dynamic instances of 
a profiling target in one program run to 
record their input sizes and costs,
and infer the whole input size and cost for all instances in the same run.}
\commentty{infer information for what?}


How we do sampling is similar to what is described in previous works 
on statistical debugging~\cite{liblit03,liblit05,CCI,SongOOPSLA2014,ldoctor}.
Given a code construct to be profiled, 
we clone the code construct.
We instrument the cloned version to record information for input size and cost.
We also dump extra delimiters to log before each 
execution of the cloned version
to differentiate information collected from different instances. 


Before each execution of a code construct under profiling, 
we choose between the cloned version and the original version. 
How many times the cloned version is executed 
depends on a tunable sampling rate. 
To make the choice between the two versions,
we add a global counter to the monitored program. 
If the counter value is larger than zero, 
we choose the original version and decrease the counter value by one.
If the counter value is equal to zero,
we choose the cloned version and reset the counter value to 
a random number, 
whose expectation is 
equal to the inverse of the sampling rate.  

We design two sampling policies. 
For the first one, we sample dynamic instances of a code construct 
independently from each other.
For the second one, we sample a pair of consecutive dynamic instances, 
with the intuition that two consecutive 
instances may share more information.


After sampling, \Tool next \emph{infers input and cost information
for all instances of the same code construct.} 
\commentty{infer the information to do what?}.  
\commentlh{
Cost can be simply inferred by multiplying recorded cost values with sampling rate.
To infer the whole input size for all instances, }
we leverage the mark-and-recapture method~\cite{mark-recapture} 
Mark-and-recapture is a commonly used statistical method 
for estimating the size of an animal population. 
In this method, some of the animals are captured, marked, and released. 
Then, another group of the animals are captured.
The size of the whole animal population is estimated 
based on the ratio of marked animals in the second captured sample. 
 

To utilize the mark-and-recapture method, 
we first process our log to calculate a set of memory cells, 
which contribute RMS
or DSS for each sampled dynamic instances. 
We then estimate the whole RMS or DSS for all instances using the following formula.
We assume we collect a sequence of $m$ samples for a profiled code construct 
in one program run.
Given the $i$th sample, we use $M_i$ to represent the 
total number of distinct memory cells in the previous $i-1$ samples, 
$C_i$ represents the number of distinct memory cells in the $i$th sample,
and $R_i$ represents the number of distinct memory cells in 
the $i$th sample that also appeared in one of the previous $i-1$ samples.
The estimated RMS or DSS of the profiled code construct is:

\begin{equation} \label{eq:mark}
N = \sum\limits_{i=1}^m M_i*C_i\Big/\sum\limits_{i=1}^m R_i
\end{equation}

\subsection{Ranking Complexity Problems}

After the profiles are collected, \Tool computes
the cost function for each unique code construct
(i.e., profile target) using the standard curve fitting 
method~\cite{curve-fitting,curve-bounding}. 

 
As we discussed in Section~\ref{subsec:existing}, 
existing techniques~\cite{Aprof1,Aprof2,AlgoProf} 
simply attribute complexity to each function, 
without providing any future analysis. 
Therefore, they fail to identify root-cause functions for 
performance failures caused by complexity problems. 

To address this problem, we design a ranking algorithm 
with the goal to identify root-cause functions\footnote{We consider a loop 
as a function called by its parent function containing the loop}. 
Our algorithm follows three intuitions. 
First, root-cause functions must have the same (or higher) 
complexity as \texttt{main} function.
Second, root-cause functions must consume large computation cost.
Third, since all direct and indirect callers of root-cause functions 
consume more cost and have the same complexity, 
we should rank callee higher than its caller, 
to reduce false positives. 

Our algorithm works as follows. 
We first filter out functions with complexity lower than \texttt{main} function.
We then compute caller-callee relationship using static analysis and tracing logs. 
If there is direct or indirect caller-callee relationship 
between function \texttt{A}
and function \texttt{B}, we will rank \texttt{B} higher than \texttt{A}.
If there is not caller-callee relationship,
we rank the one with larger cost higher.


