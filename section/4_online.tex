\section{\Tool Design and Implementation}
\label{sec:online}

%In this section, we present technical details 
%for the production-run version of \Tool, 
%including the high-level design choices
%and several optimization policies. 

Guided by the findings of empirical study in Section~\ref{sec:back},
we design and implement \Tool, a performance \commentlh{algorithmic?}
profiler to detect complexity problems. \Tool
can be used in both production environment 
and in-house development. We first describe
the production-run version of \Tool. 
The key benefits of \Tool include its \emph{low runtime} 
overhead and capability of \emph{ranking 
and localizing complexity problems in specific code constructs}. 
We then describe how to use \Tool as an in-house testing tool. 

The design of \Tool used in production environment 
follows two steps. First, given a program under profiling,
\Tool instruments the program and
distribute it to end users to collect runtime profiles
under different user inputs (e.g., workload). 
To minimize runtime overhead, we design and implement
several advanced instrumentation methods in \Tool. 
In the second step, \Tool computes cost functions for
different code constructs based on the execution profiles
and then reports a ranked list of code constructs in
terms of their likelihood of containing performance bugs due to
complexity problems.  We next describe details of the two steps. 


\subsection{Using Light-Weighted Instrumentation 
for Algorithmic Profling}

To obtain execution profiles, we need to select suitable 
\emph{profiling targets} (i.e.,
code constructs), \emph{input metrics}, and \emph{cost metrics}. 
\commentlh{
\Tool supports code constructs in any granularity. 
However, one finding in our empirical study is 
that all complexity problems are 
caused either by a loop or a recursive function. 
Therefore, we recommend code constructs that related 
to loops and recursive functions as profiling targets. 
Distinct code constructs are instrumented by the production-run
version of \Tool to create different program versions.}
As for input metrics, our study found that most two commonly involved 
data structures in complexity problems 
are array and linked list. Therefore, if a loop is chosen as a profiling target 
and it is to process an array 
or a linked list, we use data structure size (DSS) as its input metric. 
Otherwise, we use Read Memory Size (RMS), 
since RMS is more generic as we discussed in Section~\ref{sec:back}. 
The difference between DSS and RMS is that 
we only need to instrument \texttt{read} instructions accessing elements 
in an array or a linked list for DSS,  without instrument \texttt{write} instructions.
Therefore, DSS requires to record less dynamic information. 
The cost metric used in \Tool is executed basic 
blocks (BBs), 
\commentlh{
and we have two approximating metrics 
for loops and recursive functions respectively.  
}


As discussed in Section~\ref{subsec:existing}, 
existing algorithmic profiling techniques
often incur significant overhead and thus cannot be used in production
environment. To address this issue, \Tool proposes several 
methods to enable practical online instrumentation, including 1) 
minimizing instrumented code at each user's end; 
2) optimizing instrumentation sites;
3) approximation; and 4) sampling. 



\subsubsection{Creating Multiple Instrumentation Versions}

\Tool creates multiple versions of program under profiling, 
and for each version \Tool only instruments a small number of 
profiling targets. 
All different versions are distributed to different end users. 
Since there is a huge amount of end users, 
we anticipate that enough profiles can still be collected 
for each version to conduct algorithmic profiling. 

To select profiling targets, \commentty{How to determine
which code targets to instrument in each version?}
\commentlh{we are largely guided by our empirical 
study in Section~\ref{sec:tax}.
Since all complexity problems involve a 
loop or a recursive function, 
we choose loops, recursive functions, functions containing loops,
and functions invoked inside a loop directly 
or indirectly as profiling targets. 
We ignore loops whose iteration numbers are constant,
since the execution of these loops does not scale with input 
and cannot cause complexity problems.} 





\subsubsection{Optimizing Instrumentation Sites}

%We briefly overview the algorithm 
%designed in previous works~\cite{Aprof1,Aprof2} 
%before discussing our two designed optimizations. 

To measure RMS (i.e., the input metric), we need to instrument 
and the \texttt{read} and \texttt{write} instructions in every 
code construct (or profiling target). \commentty{not sure if I'm correct.}
\commentlh{correct.}
\commentlh{
Different from the algorithm described 
in previous works~\cite{Aprof1,Aprof2},
we do not maintain any global data structure,
since we want to simplify online monitoring functionalities 
and reduce runtime overhead.
Instead, we trace memory addresses 
for \texttt{read} and \texttt{write} instructions 
and we analyze tracing log offline to calculate RMS. 
}

In order to reduce the number of \texttt{read} 
and \texttt{write} instructions
to be instrumented, our approach works as follows. 
If a write on a memory cell happens earlier, 
following read on the same memory cell will not increase RMS,
\commentlh{since RMS only count memory cells whose first access is read.
Similarly, if a read on a memory cell happens earlier,
following read on the same memory cell will not increase RMS either,
since RMS only considers distinct memory cells. 
Therefore, given two consecutive memory accesses on the same location,
we do not need to instrument the second one. 
We design an intra-procedural analysis, based on dominator analysis,
which can tell us when an instruction is executed 
whether another instruction has to be executed in advance.}
We focus on stack memory cells that hold scalar variables 
and only have read and write as uses 
(i.e., not having ``address of'' as uses), 
so that alias analysis is avoided. 


\commentlh{
When counting the executed basic blocks (BBs) to compute cost metric,
a naive way is to instrument every basic block to increment a counter by one.
To reduce the number of instrumentation 
sites that update the counter,
we propose a method to selectively 
instrument part of basic blocks.}
\commentty{optimization in what way?} 
we employ an algorithm \commentty{does the algorithm
have a name?}, \commentlh{no}
which was originally designed to efficiently 
count edge events through selectively instrumenting counter update sites 
on control flow graphs (CFGs)~\cite{event-counting}.
The algorithm has already been proved to conduct path 
profiling efficiently~\cite{path-profiling,peter-ase}.  
Since the original algorithm is design to count edge events, 
we split each BB into two and label the event number to be one for 
edges connecting a pair of split BBs, 
leaving the event number as zero for other edges. 
After that, applying the algorithm~\cite{event-counting}
can tell us optimized instrumentation \commentlh{policy 
about where to update the couter and how to update the counter.} 


\subsubsection{Approximation of Input and Cost Metrics}

\commentty{Approximation of what? I think we should
make the title more complete.}

Accurately recording dynamic information for input and cost metrics
for each code construct
may incur a large runtime overhead.
Therefore, we propose two approximation mechanisms. 
First, if a loop is to process an array, 
we only record the addresses of the first 
and the last accessed elements. 
\commentty{Why?}
We calculate the difference between the two addresses and use it as DSS input metric. 
Second, instead of using executed BBs, 
we use LIs as cost metric for loops 
and use RIs for recursive functions. 
\commentty{what are LIs and RIs? what are their benefits?} 


One challenge is to \emph{identify array-processing loops}. 
%
%\noindent{\underline{How to identify an array-processing loop?}}
To address this, given a loop, we analyze all pointer usage inside the loop. 
If a pointer is deferenced in every iteration of a loop, 
and its value is also increased or decreased by 
an integer number in every iteration,
we consider the pointer points to array elements and 
the loop is an array-processing loop.  
For example, as shown in Figure~\ref{fig:mysql27287}, 
\texttt{p} points to array elements. 
It is deferenced in every iteration of the loop 
in Figure~\ref{fig:mysql27287},
and the pointer value is decreased by one in every loop iteration. 

\commentty{Why do we need to handle linked-list-processing loops?
In the first paragraph, only array-processing loop is mentioned.}
Another challenge is to \emph{identify linked-list-processing loops}.
%\noindent{\underline{How to identify a linked-list-processing loop?}}
To address this, we also analyze all pointer usage inside each loop, 
by checking whether a pointer satisfies the following three conditions.
First, its base type is a \texttt{struct}.
Second, it is deferenced in every iteration of a loop.
Third, it is updated in every iteration with a new value 
from one field of the \texttt{struct} it points to.
If a pointer satisfies the three conditions, 
we consider it points to elements in a linked list.
We also consider the loop which deferences the pointer and 
updates the pointer's value as  
a linked-list-processing loop. 



\subsubsection{Sampling}

\commentty{Again, sampling what?}

Previous work~\cite{liblit03,liblit05,CCI,SongOOPSLA2014,ldoctor}
demonstrate that sampling can greatly 
reduce overhead for dynamic techniques,
while still preserving their desired 
detection and diagnosis capability. 
This motivates us to apply sampling to algorithmic profiling. 
Our study in Section~\ref{sec:back} shows that 
the majority of complexity problems are caused by repeated execution of a loop
or a recursive function. 
This result inspires us to sample some execution of 
a loop or a recursive function,
and infer information for all executions, 
based on collected samples.
\commentty{infer information for what?}


%\noindent{\underline{How to do sampling?}}
How we do sampling is similar to what is described in previous works 
on statistical debugging~\cite{liblit03,liblit05,CCI,SongOOPSLA2014,ldoctor}.
Given a loop\footnote{We handle a recursive function 
in the same way as we handle a loop.} to be monitored, 
we clone the loop.
We instrument the cloned loop to trace memory read and write for RMS, 
or trace memory read accessing data structure elements for DSS.
To keep things simple, 
for each memory access, we record its memory address 
and whether it is a read or write directly to log, 
without maintaining any global data structure. 
We dump extra delimiters to log before each execution of the cloned loop 
to differentiate information collected 
from different samples.

Before each execution of a monitored loop, 
we choose between the cloned version and the original version. 
How many times the cloned version is executed 
depends on a tunable sampling rate. 
To make the choice between the two versions,
we add a global counter to the monitored program. 
If the counter value is larger than zero, 
we choose the original version and decrease the counter value by one.
If the counter value is equal to zero,
we choose the cloned version and reset the counter value to 
a random number, 
whose expectation is 
equal to the inverse of the sampling rate.  

We design two sampling policies. 
For the first one, we sample loop executions independently from each other.
For the second one, we sample a pair of consecutive loop executions, 
with the intuition that two consecutive 
executions may share more information.


After sampling the executions, \Tool next \emph{infers information
from all of the executions} to \commentty{infer the information to do what?}.  
 %
 To do this, we leverage the mark-and-recapture method~\cite{mark-recapture} to 
estimate RMS or DSS for all execution of a monitored loop 
based on the sampled execution. 
Mark-and-recapture is a commonly used statistical method 
for estimating the size of an animal population. 
In this method, some of the animals are captured, marked, and released. 
Then, another group of the animals are captured.
The size of the whole animal population is estimated 
based on the ratio of marked animals in the second captured sample.  

To utilize the mark-and-recapture method, 
we first process our log to calculate a set of memory cells, 
which contribute RMS
or DSS for each sample. 
We then estimate the whole RMS or DSS using the following formula.
We assume we collect a sequence of $m$ samples for a monitored loop 
in one program run.
Given the $i$th sample, we use $M_i$ to represent the 
total number of distinct memory cells in the previous $i-1$ samples, 
$C_i$ represents the number of distinct memory cells in the $i$th sample,
and $R_i$ represents the number of distinct memory cells in 
the $i$th sample that also appeared in one of the previous $i-1$ samples.
The estimated RMS or DSS of  the monitored loop is:

\begin{equation} \label{eq:mark}
N = \sum\limits_{i=1}^m M_i*C_i\Big/\sum\limits_{i=1}^m R_i
\end{equation}

\subsection{Ranking Complexity Problems}

After the profiles are collected, \Tool computes
the cost function for each unique code construct
(i.e., profile target) using the standard approach
in Section~\ref{subsec:pre}. 
 
As we discussed in Section~\ref{subsec:existing}, 
existing techniques~\cite{Aprof1,Aprof2,AlgoProf} 
simply attribute complexity to each executed function, 
without providing any future analysis. 
Therefore, they fail to identify root-cause functions for 
performance failures caused by complexity problems. 

To address this problem, we design a ranking algorithm 
with the goal to identify root-cause functions. 
Our algorithm follows three intuitions. 
First, root-cause functions must have the same (or higher) 
complexity as \texttt{main} function.
Second, root-cause functions must consume large computation cost.
Third, since all direct and indirect callers of root-cause functions 
consume more cost and have the same complexity, 
we should rank callee higher than its caller, 
to reduce false positives. 

Our algorithm works as follows. 
We first filter out functions with complexity lower than \texttt{main} function.
We then compute caller-callee relationship using static analysis and RMS logs. 
If function \texttt{A} invokes function \texttt{B}, 
we consider there is a partial order between \texttt{A} and \texttt{B},
and \texttt{B} should be ranked higher ($\texttt{B} \leq \texttt{A}$). 
In the end, we calculate a total order for all executed functions based 
on their partial orders and use the total order as our ranking list. 
If two functions are not comparable 
(neither $\texttt{A} \leq \texttt{B}$ nor $\texttt{B} \leq \texttt{A}$), 
we rank the one with larger cost higher. 
\commentty{This is difficult to understand --- I suggest we add an example}

