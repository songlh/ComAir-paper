\section{\Tool Design and Implementation}
\label{sec:online}

%In this section, we present technical details 
%for the production-run version of \Tool, 
%including the high-level design choices
%and several optimization policies. 

Guided by the findings of empirical study in Section~\ref{sec:study},
we design and implement \Tool, an algorithmic
profiler to detect complexity problems. \Tool
can be used in both production environment 
and in-house development. We first describe
the production-run version of \Tool. 
The key benefits of \Tool include its \emph{low runtime} 
overhead and capability of \emph{ranking 
and localizing complexity problems in specific code constructs}. 
We then describe how to use \Tool as an in-house testing tool. 

The design of \Tool used in production environment 
follows two steps. First, given a program under profiling,
\Tool instruments the program and
distribute it to end users to collect runtime profiles
under different user inputs (e.g., workloads). 
To minimize runtime overhead, we design and implement
several advanced instrumentation methods in \Tool. 
In the second step, \Tool computes cost functions for
different code constructs based on the execution profiles
and then reports a ranked list of code constructs in
terms of their likelihood of containing performance bugs due to
complexity problems.  
We next describe details of the two steps. 


\subsection{Using Light-Weighted Instrumentation 
for Algorithmic Profling}
\label{sec:opt}

To obtain execution profiles, we need to select suitable 
\emph{profiling targets} (i.e.,
code constructs), \emph{input metrics}, and \emph{cost metrics}. 

\Tool supports code constructs in any granularity as profiling targets. 
However, one finding (Implication 2) in our empirical study is 
that all complexity problems are 
caused either by a loop or a recursive function. 
%\commentty{Is ``ALL  complexity problems..." true? This is inconsistent
%with the data from implication 2.} 
%\commentlh{I changed the description of implication 2. }
Therefore, we consider code constructs related 
to loops and recursive functions as profiling targets. 
Distinct code constructs are instrumented by the production-run
version of \Tool to create different program versions distributed
to end users.

As for input metrics, our study found that the most two commonly involved 
data structures in complexity problems 
are array and linked list (Implication 3). 
Therefore, if a loop is chosen as a profiling target 
and it is to process an array 
or a linked list, we use Data Structure Size (DSS) as its input metric. 
Otherwise, we use Read Memory Size (RMS), 
since RMS is more generic as we discussed in Section~\ref{sec:back}. 
The difference between DSS and RMS is that 
we only need to instrument \texttt{read} instructions accessing elements 
in an array or a linked list with DSS,
while RMS needs to monitor both \texttt{read} and \texttt{write}
instructions to figure out whether a memory cell is 
firstly accessed by \texttt{read}.
Therefore, DSS requires recording less dynamic information. 


We use the executed BBs as cost metric. 
As discussed in Section~\ref{sec:back}, 
the number of executed BBs is a generic metric 
and it is more lightweight than executed instructions. 
We design a novel algorithm to reduce the number of instrumentation sites,
while still computing the precise number of executed BBs. 



As discussed in Section~\ref{subsec:existing}, 
existing algorithmic profiling techniques
often incur significant overhead and thus cannot be used in production
environment. To address this issue, \Tool proposes several 
methods to \emph{enable practical online instrumentation, which 
constitute the key novelty of \Tool over existing algorithmic profiling tools}.
These methods include 1) 
minimizing instrumented code at each user's end; 
2) optimizing instrumentation sites;
3) approximation of input metrics;
and 4) sampling profiling targets. 
We next discuss these methods in detail.


\subsubsection{Creating Multiple Instrumentation Versions}

\Tool creates multiple versions of program under profiling, 
and for each version \Tool only instruments a small number of 
profiling targets. 
All different versions are distributed to different end users. 
Since there is a huge amount of end users, 
we anticipate that enough profiles can still be collected 
for each version to conduct algorithmic profiling. 

To select profiling targets, 
%\commentty{How to determine
%which code targets to instrument in each version?}
we are largely guided by our empirical 
study in Section~\ref{sec:tax}.
Since all complexity problems involve a 
loop or a recursive function, 
we choose loops, recursive functions, functions containing loops,
and functions invoked inside a loop directly 
or indirectly as profiling targets. 
We ignore loops whose iteration numbers are constant,
since the execution of these loops does not scale with input 
and cannot cause complexity problems.
%\commentlh{

For each program version, we only instrument a small set of profiling targets
to reduce runtime overhead.
Given a instrumented profiling target,
its direct and indirect callees are also instrumented. 
Therefore, we will not put code constructs with caller-callee 
relationship in the same set. 
If there are limited program versions we can distribute at the same time,
we prioritize ``simple'' code constructs, such as 
functions without callees and inner loops of nested loops,
and we gradually select more ``complex'' code constructs along call chains, 
if we cannot find complexity problems inside simple code constructs.
%}
%\commentty{The question is how to select profiling
%targets for different versions? I assume you do not 
%instrument ALL loops or recursive functions
%for every version.}




\subsubsection{Optimizing Instrumentation Sites}

%We briefly overview the algorithm 
%designed in previous works~\cite{Aprof1,Aprof2} 
%before discussing our two designed optimizations. 

We propose two instrumentation optimization strategies for obtaining
input and cost metrics, respectively. 

\noindent{\underline{\textit{Input metric.}}
To measure RMS (i.e., the input metric), we need to instrument  
 \texttt{read} and \texttt{write} instructions in a 
code construct (or profiling target). 
Different from the algorithm described 
in previous work~\cite{Aprof1,Aprof2},
we do not maintain any global data structure,
since we want to simplify online monitoring  
and reduce runtime overhead.
Instead, we trace \texttt{read} and \texttt{write} 
instructions to record instructions' id and accessed addresses.
We then analyze tracing log offline to calculate RMS. 


In order to reduce the number of \texttt{read} 
and \texttt{write} instructions
to be instrumented, we propose two optimization algorithms. 
The first algorithm works as follows. 
If a write on a memory cell happens earlier, 
following read on the same memory cell will not increase RMS,
since RMS only counts memory cells whose first access is read.
Similarly, if a read on a memory cell happens earlier,
following read on the same memory cell will not increase RMS either,
since RMS only considers distinct memory cells. 
Therefore, given two consecutive memory accesses on the same location,
we do not need to instrument the second one. 
We design an intra-procedural analysis, based on dominator analysis,
which can tell us when an instruction is executed 
whether another instruction has to be executed in advance.
We focus on stack memory cells that hold scalar variables 
and only have read and write as uses 
(i.e., not having ``address of'' as uses), 
so that alias analysis is avoided. 
In the example of Figure~\ref{fig:mysql27287},
suppose there are four reads on variable \texttt{p} inside 
the loop --- two are at 
line 7 (``\texttt{p>=items}'' and ``\texttt{p--}''), 
and the other two are at line 8 and 
there is one write on \texttt{p} (``\texttt{p--}'').
In this case, variable \texttt{p} meets the requirement of the optimization,
because ``\texttt{p>=items}'' is conducted in the loop header 
and it dominates all other accesses. Therefore, 
we only need to instrument the read access inside ``\texttt{p>=items}''.


%\commentlh{
For the second optimization algorithm, if a read or a write is conducted inside a loop and 
the accessed address is not changed across different loop iterations,
we promote the instrumentation to the loop pre-header, 
instead of recording the accessed address in every iteration. 
For example, in Figure~\ref{fig:mysql27287}, 
the address of variable \texttt{p} is not changed 
inside the loop.
Therefore, we can record the address of \texttt{p} in the loop's 
preheader for ``\texttt{p>=items}''.
%}

\noindent{\underline{\textit{Cost metric.}}
When counting the executed basic blocks (BBs) to compute cost metric,
a naive way is to instrument every basic block to increment a counter by one.
To reduce the number of instrumentation 
sites that update the counter,
we design an algorithm to selectively 
instrument part of basic blocks.

Our algorithm leverages the previous
work on counting 
edge\footnote{An edge on CFG represents a jump from 
one basic block to another basic block.} 
events through selectively 
instrumenting counter update sites on control flow graphs (CFGs)~\cite{event-counting},
which has been proved to conduct path 
profiling efficiently~\cite{path-profiling,peter-ase}.  
However, the original algorithm cannot be
directly adopted in our case because it is designed
to count edge events, not BB events.  
To address this problem, \Tool firstly splits 
each monitored BB into two, 
and the monitored BBs include BBs inside profiling targets 
and BBs inside functions called from profiling targets.
Then, \Tool labels the event number as ``1" for 
edges connecting a pair of split BBs, 
leaving the event number as ``0" for other edges. 
After that, applying the algorithm~\cite{event-counting}
can inform \Tool  where to update 
the counter and how to update the counter.
%\commentlh{
For example, in Figure~\ref{fig:mysql27287}
there are in total six BBs inside 
the loop,
but \Tool only instruments five of them and 
updates the counter by two for one BB and by one for the other four BBs,
instead of updating the counter by one for all the six BBs.
As another example, there are 141 BBs 
inside function \texttt{mult\_alg()} in Figure~\ref{fig:gcc27733}, 
while \Tool only instruments 78 of them. 
%}

\subsubsection{Approximation of Input Metric}

Accurately recording dynamic input information
for each code construct
may still incur a large runtime overhead.
Therefore, we propose an approximation mechanism. 
If a profiled code construct is a loop and 
it is to process an array or a linked list,
we use the DSS as input metric.


To apply the approximation, we are facing two challenges. 
The first one is to \emph{identify array-processing loops}.
To address this, given a loop, we analyze all pointer usage inside the loop. 
If a pointer is deferenced in every iteration of the loop, 
and its value is also increased or decreased by 
an integer number in every iteration,
we consider the pointer points to array elements and 
the loop is an array-processing loop.  
For example, as shown in Figure~\ref{fig:mysql27287}, 
\texttt{p} points to array elements. 
It is deferenced in every iteration of the loop 
in Figure~\ref{fig:mysql27287},
and the pointer value is decreased by one in every loop iteration. 
For an array-processing loop, 
instead of recording all addresses of 
accessed array elements,
we only record the addresses 
of the first and the last accessed elements. 
We calculate the address difference and use it as DSS.
The reason is that array is organized as a 
consecutive memory, and the address difference 
between the first and last accessed elements is roughly in linear 
relationship with the number of accessed elements and DSS. 




%\commentty{Why do we need to handle linked-list-processing loops?
%In the first paragraph, only array-processing loop is mentioned.}
%\commentlh{I have changed the first paragraph in this subsection.}
The second challenge is to \emph{identify linked-list-processing loops}.
%\noindent{\underline{How to identify a linked-list-processing loop?}}
To address this, we also analyze all pointer usage inside a loop under profiling, 
by checking whether a pointer satisfies the following three conditions.
First, its base type is a \texttt{struct}.
Second, it is deferenced in every iteration of the loop.
Third, it is updated in every iteration with a new value 
from one field of the \texttt{struct} it points to.
If a pointer satisfies the three conditions, 
we consider it points to elements in a linked list.
We also consider the loop is a linked-list-processing loop. 



\subsubsection{Sampling Dynamic Instances of Profiling Targets}

%\commentty{Again, sampling what?}

Previous works~\cite{liblit03,liblit05,CCI,SongOOPSLA2014,ldoctor}
demonstrate that sampling can greatly 
reduce overhead for dynamic techniques,
while still preserving their desired 
detection and diagnosis capability. 
This motivates us to apply sampling to algorithmic profiling. 
Our study in Section~\ref{sec:study} shows that 
the majority of complexity problems are caused by repeated execution of a loop
or a recursive function. 
This finding inspires us to sample some dynamic instances of 
a profiling target in one program run to 
record their input sizes and costs,
and infer the whole input size and cost for all instances in the same run.


How we do sampling is similar to what is described in previous works 
on statistical debugging~\cite{liblit03,liblit05,CCI,SongOOPSLA2014,ldoctor}.
Given a profiling target, 
we clone it
and we instrument the cloned version to record information for input size and cost.
We also dump extra delimiters to log before each 
execution of the cloned version
to differentiate information collected from different dynamic instances. 

Before each execution of a profiling target, 
we choose between the cloned version and the original version. 
How many times the cloned version is executed 
depends on a tunable sampling rate. 
To make the choice between the two versions,
we add a global counter to the monitored program. 
If the counter value is larger than zero, 
we choose the original version and decrease the counter value by one.
If the counter value is equal to zero,
we choose the cloned version and reset the counter value to 
a random number, 
whose expectation is 
equal to the inverse of the sampling rate.  
To guarantee we have something recorded in 
each profiled program run,
we always sample the first dynamic instance. 


%We design two sampling policies. 
%For the first one, we sample dynamic instances of a code construct 
%independently from each other.
%For the second one, we sample a pair of consecutive dynamic instances, 
%with the intuition that two consecutive 
%instances may share more information.


After sampling, \Tool next \emph{infers input and cost information
for all instances of the same profiling target.} 
Cost can be simply inferred by multiplying recorded cost values with sampling rate.
To infer the whole input size for all instances, 
we leverage the mark-and-recapture method~\cite{mark-recapture}. 
Mark-and-recapture is a commonly used statistical method 
for estimating the size of an animal population. 
In this method, some of the animals are captured, marked, and released. 
Then, another group of the animals are captured.
The size of the whole animal population is estimated 
based on the ratio of marked animals in the second captured sample. 
 

To utilize the mark-and-recapture method, 
we first process our log to calculate a set of memory cells, 
which contribute RMS
or DSS for each sampled dynamic instance. 
We then estimate the whole RMS or DSS for all instances using the following formula.
We assume we collect a sequence of $m$ samples for a profiling target 
in one program run.
Given the $i$th sample, we use $M_i$ to represent the 
total number of distinct memory cells in the previous $i-1$ samples, 
$C_i$ represents the number of distinct memory cells in the $i$th sample,
and $R_i$ represents the number of distinct memory cells in 
the $i$th sample that also appeared in one of the previous $i-1$ samples.
The estimated RMS or DSS of the profiling target is:

%\vspace{-0.in}
\begin{equation} \label{eq:mark}
N = \sum\limits_{i=1}^m M_i*C_i\Big/\sum\limits_{i=1}^m R_i
\end{equation}

\subsection{Ranking Complexity Problems}

After the profiles are collected, \Tool computes
the cost function for each unique code construct
(i.e., profile target) using the standard curve fitting 
method~\cite{curve-fitting,curve-bounding}. 

 
As we discussed in Section~\ref{subsec:existing}, 
existing techniques~\cite{Aprof1,Aprof2,AlgoProf} 
simply attribute complexity to each function, 
without providing any future analysis. 
Therefore, they fail to identify root causes for 
performance failures caused by complexity problems. 
%\commentty{Does \Tool rank functions or code constructs?
%We have been measuring costs for code constructs...}
%\commentlh{addressed.}

To address this problem, we design a ranking algorithm 
with the goal to identify root-cause code constructs.
Our algorithm follows three intuitions. 
First, root-cause code constructs must have highest complexity.
Second, root-cause code constructs must consume large computation cost.
Third, since all direct and indirect callers of root-cause code constructs 
consume more cost and may have the same complexity, 
we should rank callee higher than its caller, 
to reduce false positives. 


Our algorithm works as follows. 
According to their complexity, 
we first divide profiling targets into three groups, 
exponential-complexity group, 
polynomial-complexity group and linear-complexity group.
We always rank profiling targets with exponential complexity highest,
profiling targets with polynomial complexity in the middle,
and profiling targets with linear complexity lowest. 
For profiling targets within the same group, 
we then compute caller-callee relationship using static analysis and tracing logs.
Since we record instructions' ids, tracing logs can help us solve function pointers. 
If there is direct or indirect caller-callee relationship 
between profiling target \texttt{A}
and profiling target \texttt{B}, we rank \texttt{B} higher than \texttt{A}.
Since a root-cause profiling target may 
cause all its callers to be in the same complexity, 
prioritizing callees over callers can help reduce false positives.
%\commentty{need a bit explanation on why the caller-callee relationship matters.}
If there is no caller-callee relationship,
we rank the one with larger observed cost higher.


%\commentty{How to rank two functions with different complexities? What
%about linear complexity?}
%\commentlh{addressed}

To illustrate our ranking algorithm, suppose we have four profiling targets, 
whose target ids (0 to 3), complexity and ever observed largest costs are listed as follows:
(0, $O(n)$, 2000), (1, $O(n^2)$, 800), (2, $O(n^2)$, 1200), and (3, $O(n^2)$, 300).
We also assume target 2 is an indirect caller of target 1 and target 3,
and there is no caller-callee relationship between target 1 and target 3.
According to our algorithm, 
we rank the four profiling targets as 1, 3, 2, and 0. 
Targets 1, 2 and 3 have higher complexity than target 0, 
so that these three are ranked higher.
Target 2 is has caller-callee relationship with targets 1 and 3,
and thus it is ranked lower. 
Target 1 has larger cost than target 3, so that it is ranked as No. 1. 



%\commentty{If we have time, section 4.2 can be expanded, since
%we claim it as one of our contributions. The current version is a bit short. 
%Use a concrete example might help, e.g., ``suppose there four code constructs
%with different cost functions...".  }
%\commentlh{addressed}