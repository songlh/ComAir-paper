\section{Online Algorithmic Profiling}
\label{sec:online}

In this section, we present technical details 
for the production-run version of \Tool, 
including the high-level design choices
and several optimization policies. 

We envision \Tool to be used in the following way 
under the production-run setting.
Given a monitored program, \Tool creates multiple versions of it, 
and for each version \Tool only instruments a small piece of code. 
All different versions are distributed to different end users. 
Since there is a huge amount of end users, 
we anticipate that enough profiles can still be collected 
for each version 
to conduct algorithmic profiling. 
 
\subsection{High-level Design}
\label{sec:high}

We design the production-run version 
of \Tool following three principles. 

\noindent\underline{Study-guided design}.
We leverage the results of our empirical study in Section~\ref{sec:back},
to select profiling targets and suitable input metrics.
One finding of our study is that
all complexity problems are 
caused either by a loop or a recursive function. 
Therefore, the production-run version of \Tool chooses loops 
and recursive functions as profiling targets, 
and it instruments distinct loops or recursive functions to create multiple
versions of a monitored program. 

Another important finding of our study
is that the most two commonly involved 
data structures in complexity problems 
are array and linked list.
Therefore, if a loop is to process an array 
or a linked list, we use DSS as its input metric.
Otherwise, we use RMS. 
The difference between DSS and RMS is that 
we only need to instrument \texttt{read} instructions 
accessing elements 
in an array or a linked list for DSS, 
without instrument \texttt{write} instructions.
Therefore, DSS requires to 
record less dynamic information. 

\noindent\underline{Approximation}.
Accurately recording dynamic information for input and cost metrics
may incur a large runtime overhead.
Therefore, we propose two approximation mechanisms. 
First, if a loop is to process an array, 
we only record the addresses of the first 
and the last accessed elements.
We calculate the difference between the two addresses, 
and use it as DSS.
Second, instead of using executed BBs, 
we use LIs as cost metric for loops 
and use RIs for recursive functions. 

\noindent\underline{Sampling}.
Previous works~\cite{liblit03,liblit05,CCI,SongOOPSLA2014,ldoctor}
demonstrate that sampling can greatly 
reduce overhead for dynamic techniques,
while still preserving their desired 
detection and diagnosis capability. 
These previous work motivate us to apply sampling to algorithmic profiling. 
Our study in Section~\ref{sec:back} shows that 
the majority of complexity problems are caused by repeated execution of a loop
or a recursive function. 
This result inspires us to sample some execution of 
a loop or a recursive function,
and infer information for all execution, 
based on collected samples.

\subsection{Detailed Implementation}

\noindent{\underline{How to identify an array-processing loop?}}
Given a loop, we analyze all pointer usage inside the loop. 
If a pointer is deferenced in every iteration of a loop, 
and its value is also increased or decreased by 
an integer number in every iteration,
we consider the pointer points to array elements and 
the loop is an array-processing loop.  
For example, as shown in Figure~\ref{fig:mysql27287}, 
\texttt{p} points to array elements. 
It is deferenced in every iteration of the loop 
in Figure~\ref{fig:mysql27287},
and the pointer value is decreased by one in every loop iteration. 

\noindent{\underline{How to identify a linked-list-processing loop?}}
We also analyze all pointer usage inside each loop, 
by checking whether a pointer satisfies the following three conditions.
First, its base type is a \texttt{struct}.
Second, it is deferenced in every iteration of a loop.
Third, it is updated in every iteration with a new value 
from one field of the \texttt{struct} it points to.
If a pointer satisfies the three conditions, 
we consider it points to elements in a linked list.
We also consider the loop which deferences the pointer and 
updates the pointer's value as  
a linked-list-processing loop. 


\noindent{\underline{How to do sampling?}}
How we do sampling is similar to what is described in previous works 
on statistical debugging~\cite{liblit03,liblit05,CCI,SongOOPSLA2014,ldoctor}.
Given a loop\footnote{We handle a recursive function 
in the same way as we handle a loop.} to be monitored, 
we clone the loop.
We instrument the cloned loop to trace memory read and write for RMS, 
or trace memory read accessing data structure elements for DSS.
To keep things simple, 
for each memory access, we record its memory address 
and whether it is a read or write directly to log, 
without maintaining any global data structure. 
We dump extra delimiters to log before each execution of the cloned loop 
to differentiate information collected 
from different samples.

Before each execution of a monitored loop, 
we choose between the cloned version and the original version. 
How many times the cloned version is executed 
depends on a tunable sampling rate. 
To make the choice between the two versions,
we add a global counter to the monitored program. 
If the counter value is larger than zero, 
we choose the original version and decrease the counter value by one.
If the counter value is equal to zero,
we choose the cloned version and reset the counter value to 
a random number, 
whose expectation is 
equal to the inverse of the sampling rate.  

We design two sampling policies. 
For the first one, we sample loop executions independently from each other.
For the second one, we sample a pair of consecutive loop executions, 
with the intuition that two consecutive 
executions may share more information.

\noindent{\underline{How to infer all information?}}
We leverage the mark-and-recapture method~\cite{mark-recapture} to 
estimate RMS or DSS for all execution of a monitored loop 
based on the sampled execution. 
Mark-and-recapture is a commonly used statistical method 
for estimating the size of an animal population. 
In this method, some of the animals are captured, marked, and released. 
Then, another group of the animals are captured.
The size of the whole animal population is estimated 
based on the ratio of marked animals in the second captured sample.  

To utilize the mark-and-recapture method, 
we first process our log to calculate a set of memory cells, 
which contribute RMS
or DSS for each sample. 
We then estimate the whole RMS or DSS using the following formula.
We assume we collect a sequence of $m$ samples for a monitored loop 
in one program run.
Given the $i$th sample, we use $M_i$ to represent the 
total number of distinct memory cells in the previous $i-1$ samples, 
$C_i$ represents the number of distinct memory cells in the $i$th sample,
and $R_i$ represents the number of distinct memory cells in 
the $i$th sample that also appeared in one of the previous $i-1$ samples.
The estimated RMS or DSS of  the monitored loop is:

\begin{equation} \label{eq:mark}
N = \sum\limits_{i=1}^m M_i*C_i\Big/\sum\limits_{i=1}^m R_i
\end{equation}
