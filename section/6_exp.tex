\section{Evaluation}
\label{sec:eva}

%\subsubsection{Research Questions}

\subsection{Methodology}
We will conduct experiments to answer the following two research questions:

{\color{red} Todo: Capability: whether \Tool can report correct complexity information}

{\color{red} Todo: Accuracy: compare with in-house version what is the difference}

{\color{red} Todo: Performance: runtime overhead}

{\color{red} Todo: latency}

\begin{itemize}



\item {\bf RQ1.} 
Can sampling lower the runtime overhead while giving the same profiling results? 
A positive answer means the effectiveness of the production-run version of \Tool. 

\item {\bf RQ2.} 
Will sampling increase the profiling latency? 
By applying sampling, less information is collected in one single run. 
If we have to monitor more program runs to get the same profiling results,
the profiling latency is increased.
As we discussed earlier, the majority of complexity problems are caused 
by repeated executions of a loop or a recursive function.
It is possible that sampling can still collect enough information in one program run, 
while not increasing the profiling latency. 


\end{itemize}

\noindent\textbf{Benchmarks and Inputs}

{\color{red} list benchmark information here}

We reuse all benchmarks in Table~\ref{tab:benchmark_info}.
We monitor the loop or the recursive function with the most iterations\footnote{We consider a recursive function call instance as a loop iteration.} 
for all benchmarks,
except for Mozilla\#347306 and GCC\#46401. 
%These two bugs are caused repeated execution of a loop, 
%whose total iteration numbers ranked 2nd. 
%Instead, we monitor these two loops.
These two bugs are caused by the repeated executions of a loop with the second
highest number of iterations. Instead of using the loop with the most iterations,
we monitor these two loops.


For each benchmark, 
we use the same methodology described in Section~\ref{sec:inhouse_exp} 
to generate a sequence of inputs, 
so that the difference between the sizes of two consecutive inputs is constant.
This input set contains controlled inputs.
We also randomly sample controlled inputs to generate a set of random 
inputs to emulate the uncontrolled inputs in production runs.    

\noindent\textbf{Metrics}
We will measure the following three metrics during our experiments:
1) runtime overhead, which is the slowdown caused 
by sampling information from each program run;
2) profiling capability, which is measured as the similarity between two cost functions 
inferred under the in-house setting and the production-run setting using the same metrics;
and 3) profiling latency, which is measured by how many program 
runs are needed for the algorithmic profiling. 


\input{section/tab_online1}

\input{section/tab_online2}

\noindent\textbf{Settings}
By default, we set the sampling rate to be $1$ out of $100$ 
and run the algorithmic profiling using $100$ program runs with controlled inputs.  

Besides the default setting,
we evaluate the impact of different numbers of monitored program runs, 
ranging from $10$ to $1000$, under the default sampling rate.
We conduct this experiment by using both controlled inputs and random inputs 
to understand whether different input sets will influence the experimental results.
We also evaluate the impact of changing the sampling rate, ranging from $1$ out of $10$ to $1$ out of $10000$, 
using $100$ program runs with controlled inputs.  

Sampling will introduce some randomness. 
We conduct each experiment multiple times, 
and our results are stable across multiple experiments. 
In particular, overhead is measured using $10$ runs under the same setting. 

\noindent\textbf{Runtime Overhead}
Table~\ref{tab:overhead} shows that the runtime overhead is small under the 
default sampling rate (1 out of 100).
It is below $5\%$ in $34$ out of $38$ cases, 
and it is below $10\%$ in $35$ out of $38$ cases. 
It is below $5\%$ for all real complexity problems, 
except GCC\#1687. 

The runtime overhead is influenced by the sampling rate, 
as shown in Figure~\ref{tab:sampling}.
We can lower the runtime overhead by decreasing the sampling rate.
The overhead can be lowered to be mostly ($30/38$) 
under $1\%$ when the sampling rate is $\frac{1}{10^4}$.
The runtime overhead increases, 
if we sample more frequently.
The runtime overhead will be more than $10\%$ for most cases ($30/38$), 
when the sampling rate is $\frac{1}{10}$.

\noindent\textbf{Profiling Capability}
As shown in Table~\ref{tab:overhead}, 
with $1000$ program runs and controlled inputs, 
sampling does very little damage to the profiling capability. 
We fail to profile Mozilla\#490742 and Apache\#37184.
These two bugs are in $O(N)$ complexity. 
Their monitored loops execute only once 
and we fail to collect any sample.
For all other bugs, 
%the calculated similarity by using Equation~\ref{eq:sim}
%for the two inferred functions under 
%in-house and production-run setting
%is constantly larger than $0.85$.
the similarity calculated with Equation~\ref{eq:sim} for the two functions with the same input and cost metrics
under the in-house setting and the production-run setting is consistently larger than $0.85$.

As expected, the profiling capability will decrease when using sparser sampling rate. 
As shown in Table~\ref{tab:sampling}, 
when using the default 100 program runs,
the profiling capability is the same for the $\frac{1}{10}$ sampling rate 
and the $\frac{1}{10^2}$ sample rate.
The profiling capability will drop with the $\frac{1}{10^3}$ sampling rate. 
Nine benchmarks that can be accurately profiled with a higher sampling rate
cannot be profiled with the $\frac{1}{10^3}$ sampling rate.
The profiling capability drops further with the $\frac{1}{10^4}$ sampling rate. 
Only six benchmarks can be accurately profiled with the $\frac{1}{10^4}$ sampling rate. 
More program runs are needed for the $\frac{1}{10^3}$ sampling rate 
and the $\frac{1}{10^4}$ sampling rate. 

As shown in Table~\ref{tab:sampling}, 
using different inputs has a slight impact on the profiling capability. 
For $500$ and $1000$ program runs, 
the profiling capability is the same between controlled inputs and random inputs. 
For $100$ program runs, 
the profiling capability is better when using controlled inputs.
Two benchmarks that can be accurately profiled with controlled inputs 
cannot be accurately profiled with random inputs.
For $10$ program runs, 
some benchmarks can be accurately 
profiled with one input set, but they cannot 
be accurately profiled with the other, 
for both controlled inputs and random inputs.

\noindent\textbf{Profiling Latency}
Table~\ref{tab:sampling} shows the quantitative measurements for the impact of sampling on the profiling latency.
As we can see, with controlled inputs, 
three benchmarks need around 100 program runs for an accurate profile. 
Mozilla\#490742 and Apache\#37184 need more than 1000 runs for an accurate profile to be produced. 
With random inputs, two benchmarks, GCC\#1687 and GCC\#27733, 
need around 100 runs for an accurate profile to be produced. 
Apache\#29743 needs around 500 runs. 
Mozilla\#490742 and Apache\#37184 also need more than 1000 runs.   
This means longer profiling latencies than the in-house version of \Tool,
which needs only 10 runs for an accurate profile. 

Sampling does not lengthen the profiling latency for 32 benchmarks 
with controlled 
inputs and for 33 benchmarks with random inputs.
As we discussed earlier, these bugs are caused by repeated executions of a loop. 
Even though we apply sampling, 
we can still collect enough information to make an accurate estimate for the whole execution. 
Consequently, sampling allows us to achieve a low runtime overhead, 
accurate profiling results, and a low profiling latency at the same time. 
For other benchmarks, even though we use the $\frac{1}{10^2}$ sample rate, 
sampling does not lengthen the profiling latency by 100 times. 
For most benchmarks, 100 runs are enough for an accurate profile, 
since the root-cause loop or recursive function is executed many times, 