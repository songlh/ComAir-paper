%\newpage

\section{Related Works}
\label{sec:related}

\noindent\textbf{Empirical studies on performance problems.}
Many characteristics studies are conducted on different types 
of real-world performance
bugs~\cite{PerfBug,SongOOPSLA2014,ldoctor,Zaman2012MSR,Nistor2013MSR,HuangRegression,SmartphoneStudy,junwen-1, junwen-2}.
Similar to our work in Section~\ref{sec:study},
these studies have important findings and 
implications 
that can guide technical design to combat
performance bugs.
However, our work is the first empirical study focusing 
on complexity problems,
and it provides important supplement to existing studies.


\noindent\textbf{Traditional and algorithmic profilers.}
Traditional profilers are the most widely used tools
during performance optimization and debugging.
After collecting runtime information,
traditional profilers associate performance metrics to executed instructions,
functions, or 
calling context~\cite{oprofile,gprof,CCT,4Profilers,LagHunter,AppInsight,AdaptiveBurst,HotCallingContext}.
However, traditional profilers can only analyze one single program run,
but cannot connect results from multiple runs and cannot
predict results for inputs not seen before.
Aprof~\cite{Aprof1, Aprof2} and AlgoProf~\cite{AlgoProf} are existing
algorithmic profiling techniques.
As discussed in Section~\ref{sec:back},
these two techniques suffer from three limitations. 
\Tool enhances existing techniques through significantly 
reducing runtime overhead and providing an effective ranking mechanism. 
\Tool can accurately point out root causes 
and can be deployed in production runs. 

\noindent\textbf{Performance bug detection.}
There are many performance bug detectors, 
leveraging static or dynamic techniques to
identify bugs matching specific inefficiency
patterns~\cite{yufei-perf,CLARITY,xiao13:context,PerfBug,Alabama,CARAMEL,XuDataStructure,XuBloatPLDI2009,XuBloatPLDI2010,Cachetor,LoopInvariant,falsesharing}.
Some detected bugs by them overlap with complexity problems.
For example, 
all confirmed performance bugs detected by Toddler are 
in $O(N^2)$ complexity and used in our evaluation in Section~\ref{sec:eva}.
However, static detectors cannot provide any indication
about bugs' performance impact.
Dynamic detectors usually incur more than $10\times$ overhead,
and cannot be applied in production environments.
\Tool can detect complexity problems in production runs 
and also provide information about workloads and bugs' impact 
to help developers make better priority decisions during bug fixing. 