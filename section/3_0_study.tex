\section{Understanding Real-World Complexity Problems}
\label{sec:study}


In this section, we present an empirical study on real-world 
complexity problems. Our empirical study is conducted in two steps.
First, we quantitatively compare complexity problems with non-complexity problems
from a public performance-bug benchmark set~\cite{PerfBug,SongOOPSLA2014,ldoctor}. 
%We want to understand the popularity of complexity problems 
%and whether they are different from non-complexity 
%problems during reporting and diagnosis. 
Second, we build a taxonomy for complexity problems. 
%For bugs in each category, 
%we study what code constructs involve complexity problems,
%how the slowdown is caused and perceived by end users, 
%and the fixing strategies. 



\subsection{Quantitative Comparison}
\label{sec:compare}

The benchmark set contains 65 user-perceived performance bugs 
collected from five large, mature applications with different functionalities 
and implemented in different programming languages. 
All bugs in the benchmark suite are perceived and reported by real-world users
and have large performance impact. 
After carefully analyzing the bug reports and the associated buggy code fragments
for bugs in the benchmark set,
we identify \ComBugs performance bugs 
related to complexity problems (or complexity bugs). 
Developers usually fix these bugs by applying optimized algorithms to lower complexity
or to reduce workloads processed by the buggy code. 
These results indicate that \emph{complexity bugs are popular and 
indeed one of the major reasons causing software slowdown.}


Performance bugs not related to algorithmic complexity are caused by various reasons.
For example, several Mozilla bugs are due to misuse GUI operations, 
such as drawing transparent figures or refreshing web pages too frequently. 
There are also bugs caused by misusing synchronizations, 
like using a busy wait or conducting I/O in critical sections. 
Table~\ref{tab:study} shows the numbers of complexity bugs and non-complexity bugs.




%We conduct hypothesis t-testing~\cite{ttest} 
%to compare complexity problems and non-complexity 
%problems from the following aspects: why they are introduced, 
%how they are reported, and how they are diagnosed. 
%We use 95\% as our statistical confidence. 

%We further analyze the causes of the 30 complexity problems. 
%We discover that  misunderstanding workloads and misusing APIs
%are two major reasons why performance bugs are introduced~\cite{PerfBug}. 
%Among the 30 complexity bugs, 20 bugs are caused 
%by misunderstanding workloads
%and 8 bugs are caused by misusing APIs. 
%The two numbers for non-complexity bugs are 18 and 7 respectively. 
%There is no significant difference between complexity 
%problems and non-complexity problems. 


We next examine how long it takes to resolve performance bugs. 
On average, developers use 162 days to fix a 
complexity problem and use 103 days to fix a non-complexity problem, 
indicating that it is potentially more difficult to fix a complexity problem.


We also find that 
when reporting a performance bug, the end user often 
1) compares the application's performance 
using inputs with different sizes~\cite{SongOOPSLA2014}, 
or 2) provides an input with repetitive patterns. 
There are 25 complexity bugs in one of the two cases, 
while for non-complexity problems, the number is only 8.


\input{section/tab_study}
