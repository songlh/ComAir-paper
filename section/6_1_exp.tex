\subsection{Experimental Results}
\label{sec:results}

All experimental results are shown in Table~\ref{tab:benchmark_info}.
We discuss how these results can answer the previous three research questions as follows.


\subsubsection{Effectiveness}
As shown by the ranking numbers in Table~\ref{tab:benchmark_info},
\Tool can effectively identify buggy loops and buggy functions 
among all evaluated loops and executed functions. 
\Tool ranked the buggy loop or the buggy function as No. 1 
for \textbf{all} benchmarks under both the production-run and in-house settings. 
We also calculate the percentage of loops or functions 
to be examined before the root-cause one is identified, 
after referring our ranking list. 
Since the root-cause one is always ranked as No. 1,
the percentage is controlled by the number of evaluated loops 
or the number of executed functions. 
For the production version, the percentage is $20\%$ for most benchmarks, 
since we at most evaluate five loops. 
The percentage for Apache\#37184 is the largest one, 
because there are only three executed loops. 
For the in-house version, the minimum percentage is for GCC\#46401, 
and there are in total 4249 executed functions for this bug.
The largest one is for Mozilla\#477564. 
Overall, the percentage is less than 10\% for 33 out of 39 bugs. 
\commentty{If you rank the ground-truth as No. 1, shouldn't the 
number of loops or functions to be examined before the root 
cause is 0, so the percentage is 0? }

Our ranking mechanism can indeed save developers' efforts 
and help identify root causes. 
Take GCC\#8805 as an example.
This bug is caused by a nested loop, 
whose inner loop's total iterations are in polynomial complexity (e.g., $O(N^2)$).
To fix this bug, GCC developers significantly reduce 
the workload processed by the inner loop. 
Among the five evaluated loops, {\color{red} XX} of them are in superlinear complexity, 
and the production-run version successfully rank the inner loop as No. 1.
There are in total 1373 executed function during buggy runs,
and 87 of them are in superlinear complexity. 
The buggy function containing the outer loop is ranked as No. 1 
by the in-house version. 
By referring the results of \Tool, 
developers can avoid manually inspecting the large number of  
suspicious loops or functions in superlinear complexity. 

\subsubsection{Accuracy}
As shown in Table~\ref{tab:benchmark_info} (the ``Cost Function'' column),
the production-run version of \Tool successfully 
report the correct complexity for the buggy loop of 
each evaluated benchmark. 
We also compute the $R^2$ value between observe cost values and 
cost values predicted by the inferred cost function for each bug.  
There are only four bugs, whose computed $R^2$ value is less than $0.99$.
The minimum $R^2$ value is 0.927 for Collections\#429-1. 
Previous work consider $R^2$ value larger than 0.92 
as a good fitting~\cite{rsquare-value}.
Our results show that inferred cost functions can well represent 
observed (input size, cost) pairs. 
\commentty{should say something like the production-version
is almost as accurate as the in-house version.}

To understand whether sampling and approximation leveraged 
in the production-run version of \Tool will hurt accuracy, 
we compare reported input sizes and cost values by the two versions of \Tool.
The computed $R^2$ values are shown in Table~\ref{tab:benchmark_info} 
(the columns labeled with ``P-vs.-I''). 
For 31 out of 38 benchmarks, 
the $R^2$ values are larger than $0.99$ for both input sizes and cost values. 
For the left seven benchmarks, the $R^2$ values are all larger than $0.9$.
The minimum $R^2$ value is $0.92$ for Collections\#429-0. 
To separately inspect the effect of sampling, 
we also calculate $R^2$ values for input sizes and cost values reported 
by the production-run version under the default sampling rate and disabling sampling. 
The minimum $R^2$ value is $0.93$. 
These results demonstrate that sampling and approximation in the production-run 
version does not hurt profiling accuracy. 


\subsubsection{Overhead}
The runtime overhead for the production-run version of \Tool is 
shown in Table~\ref{tab:benchmark_info}. 
In general, the performance is good. The runtime overhead is constantly 
under 5\% for all evaluated bugs. 
For nine bugs, the overhead is less than 1\%. 
The largest overhead is 4.82\% for Collections\#434. 
We also measure the overhead for the in-house version as a comparison. 
The overhead can be as large as 1252X for Mozilla\#416628.
For 31 out of 38 bugs, the overhead is larger than 10X. 
These results show that the instrumentation mechanisms 
discussed in Section~\ref{sec:opt} can effectively reduce runtime overhead. 



\subsection{Discussion and Limitations}

\commentty{Consider adding a section
on limitations and threats to validity after the results. People would want to see under what conditions
\Tool works and does not work.}

