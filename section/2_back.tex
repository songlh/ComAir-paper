
\section{Background and Limitations}
\label{sec:back}

In this section, we discuss existing algorithmic profiling techniques 
and their limitations. 


\subsection{Existing Techniques}
To conduct algorithmic profiling for a given code construct,
we need to record input and cost information in multiple program runs. 
Then we plot recorded information with input size as x-axis and cost as y-axis.
In the end, we infer complexity for the code construct as a cost function of input size. 
To conduct algorithmic profiling effectively, 
we need to have suitable metrics for input and cost. 

\noindent{\underline{Input Metric.}}
\citet{Aprof1} proposes Read Memory Size (RMS) as as input metric. 
Given a dynamic instance of a code construct,
Aprof records its RMS as the number of distinct memory cells 
whose first access is read. 
Both reads conducted by the code construct directly and 
reads conducted by callees from the code construct are considered. 
In their following-up work~\cite{Aprof2}, 
RMS is extended to be DRMS to handle inputs from library calls 
or generated by different threads. 

Take MySQL\#27287 as an example.
RMS (or DRMS) for an execution of
the buggy loop in Figure~\ref{fig:mysql27287}
is roughly equal to 2 times the number of \texttt{XML\_NODE}s 
accessed during the execution, 
since different \texttt{XML\_NODE}s' \texttt{level} and \texttt{type} 
fields are read in different loop iterations. 
Although variable \texttt{p} and \texttt{level} are also read in each iteration,
RMS only considers distinct memory cells and 
only increments its value for the first read on these 
two variables in the first iteration. 
For the outer loop, which is not shown in Figure~\ref{fig:mysql27287}, 
its RMS is roughly equal to the size of \texttt{items}, 
since the outer loop invokes \texttt{xml\_parent\_tag()} for every
\texttt{XML\_NODE} in the \texttt{items} and RMS only 
counts distinct memory cells. 

AlgoProf~\citep{AlgoProf} uses data structure size (DSS) 
as input size for a dynamic instance of a code construct.
DSS is measured as the number of distinct accessed elements of a data structure.  
If we focus on array \texttt{items},
DSS for one execution of the inner loop is the number of accessed array elements during 
the execution. 
The outer loop's DSS is the size of \texttt{items}, 
since it invokes \texttt{xml\_parent\_tag()} for every element in \texttt{items}.

\noindent{\underline{Cost Metric.}}
When a code construct is executed,
it could consume many different types of resources, 
such as CPU cycles, network bandwidth, storage, and so on.
In this paper, focus on CPU cycles (or computation cost) 
and leave the measurement for other types of 
consumed resources for future works. 

Computation cost can be measured 
as the number of executed basic blocks (BBs)~\cite{Aprof1,Aprof2}.
The underlying assumption is that each executed 
BB will roughly consume the same CPU cycles. 
Executed instructions can also be used as cost metric. 
The number of executed instructions is highly correlated with executed BBs.
For a buggy loop, the number of loop iterations (LIs) can represent 
the computation cost of a dynamic loop instance.
Given a recursive function, the number of recursive invocations (RIs) 
of the function can serve as execution cost.
Similarly to BBs, if we use LIs and RIs, 
we assume that each loop iteration or recursive invocation 
roughly consumes the same computation cost. 

\subsection{Limitations}
\label{sec:limit}
There are three limitations in existing techniques. 

First, existing techniques can only 
attribute complexity to every executed code construct, 
while cannot provide more information about 
which code construct is the root cause 
for a performance slowdown. 
For example, after applying Aprof~\cite{Aprof1,Aprof2} 
to the performance bug in Figure~\ref{fig:mysql27287}, 
there are {\color{red} XXX} functions identified 
as having super-linear complexity. 
If we simply rank reported functions using their costs, 
\texttt{main} function will always be ranked as No.1. 
More advanced techniques are needed to tell developers 
which function is the root cause from profiling results of existing techniques. 

Second, existing techniques incur a very large runtime overhead. 
In our evaluation, we implement RMS and DSS to collect input size information. 
For most benchmarks, RMS incurs 50$X$ runtime overhead. 
DSS collects less information, and for most benchmarks, it incurs 20$X$ runtime overhead. 
Due to the extremely large overhead, 
we cannot deploy these two techniques in production-run scenario.   

Third, existing techniques do not cooperate well with sampling. 
Sampling is a widely-used technique to lower runtime overhead 
and deploy existing techniques to production runs~\cite{SongOOPSLA2014,liblit03,liblit05,CCI}. 
However, it is very difficult to apply sampling to existing algorithmic profiling techniques. 
Take MySQL\#27287 in Figure~\ref{fig:mysql27287} as an example,
the $n^2$ complexity can only be observed on the side of the outer loop. 
Each execution of the outer loop contributes almost all 
the execution time during performance failure runs. 
If we sample the outer loop, 
we may not get any sample and miss the opportunity to 
identify the $n^2$ complexity, or we get a sample and have to 
record all 
information for the sampled execution of the outer loop, 
still having a non-tolerable overhead. 
