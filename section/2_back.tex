
\section{Background}
\label{sec:back}

In this section, we present preliminaries of 
algorithmic profiling and discuss existing techniques and their
limitations. 

\commentty{I re-organized this section. Feel free to revert it if it does not make sense.}

\subsection{Preliminaries}
\label{subsec:pre}

To conduct algorithmic profiling for a given code construct,
we first need to record input and cost information in multiple program runs. 
We then plot the recorded information with input size as x-axis and cost as y-axis.
In the end, we infer complexity for the code construct as a cost function of input size. 
\commentty{To help reviewers better understand algorithmic profiling,
we can rephrase the example of Figure 1 to illustrate algorithmic profiling and
show  how the cost function looks like.}
To conduct algorithmic profiling effectively, 
we need to define suitable metrics for input and cost. 

\noindent{\underline{Input Metric.}}
%
\commentty{Can we define input metric in general? 
I feel this part should be aligned with ``Cost Metric"
below.}

Read Memory Size (RMS)~\citep{Aprof1,Aprof2}  and Data Structure Size (DSS)~\citep{AlgoProf} 
are two commonly used input metrics. 
RMS is measured as the the number of distinct memory cells 
whose first access is read. Both reads executed by the code 
construct directly and  reads executed by the callees from 
the code construct are considered. 

%Aprof \citep{Aprof1} uses Read Memory Size (RMS) as an input metric. 
%Given a dynamic instance of a code construct,
%Aprof records its RMS as the number of distinct memory cells 
%whose first access is read. 
%Both reads executed by the code construct directly and 
%reads executed by the callees from the code construct are considered. 
%In their follow-up work~\cite{Aprof2}, 
%RMS is extended  to handle inputs from library calls 
%and from different threads. 

Take MySQL\#27287 (Figure~\ref{fig:mysql27287}) as an example, 
during an execution of the buggy loop, the number of accesses
computed by RMS is approximately 
two times as the number of actual accesses. 
\commentty{I rephrased the above sentence, but am not sure if I messed it up.}
This is because different \texttt{XML\_NODE}s' \texttt{level} and \texttt{type} 
fields are read in different loop iterations. 
Although variable \texttt{p} and \texttt{level} are also read in each iteration,
RMS only considers distinct memory cells and 
only increments its value for the first read on these 
two variables in the first iteration. 
For the outer loop, which is not shown in Figure~\ref{fig:mysql27287}, 
its RMS is approximately equal to the size of \texttt{items}, 
because the outer loop invokes \texttt{xml\_parent\_tag()} for every
\texttt{XML\_NODE} in the \texttt{items} and RMS only 
counts distinct memory cells. 


Data Structure Size (DSS) is measured as
the number of distinct accessed elements of a data structure.  
%
%AlgoProf~\citep{AlgoProf} uses data structure size (DSS) 
%as input size for each dynamic instance of a code construct.
%DSS is measured as the number of distinct accessed elements of a data structure.  
In Figure~\ref{fig:mysql27287}, if we focus on array \texttt{items},
in one execution of the inner loop, the input size computed by DSS 
is equal to the number of accessed array elements. 
As for the outer loop, the input size computed by DSS  is
equal to the size of  \texttt{items}, because
it invokes \texttt{xml\_parent\_tag()} for every element in \texttt{items}.


\noindent{\underline{Cost Metric.}}
When a code construct is executed,
it could consume many different types of resources, 
such as CPU cycles, network bandwidth, storage, and so on.
In this paper, focus on CPU cycles (or computation cost) 
and leave the measurement for other types of 
consumed resources for future work.
\commentty{Any justification why CPU cycles is more
beneficial or commonly used? We could add some
citations.} 

Computation cost can be measured 
as the number of executed basic blocks (BBs)~\cite{Aprof1,Aprof2}.
The underlying assumption is that each executed 
BB will roughly consume the same CPU cycles. 
Executed instructions can also be used as cost metric. 
The number of executed instructions is highly correlated with executed BBs.
For a buggy loop, the number of loop iterations (LIs) can represent 
the computation cost of a dynamic loop instance.
Given a recursive function, the number of recursive invocations (RIs) 
of the function can serve as execution cost.
Similarly to BBs, if we use LIs and RIs, 
we assume that each loop iteration or recursive invocation 
roughly consumes the same computation cost. 



\subsection{Limitations in Existing Techniques}
\label{subsec:existing}

\commentty{If space not permitted, we can merge
this subsection with introduction.}

%
There are at least three limitations in existing algorithmic profiling techniques. 
%
First, existing techniques can only 
attribute complexity to every executed code construct
in single executions, 
while cannot provide more information about 
which code construct is the root cause 
for a performance slowdown obtained from multiple 
execution profiles. 
\commentty{Please double check the above sentence.}
For example, after applying Aprof~\cite{Aprof1,Aprof2} 
to the performance bug in Figure~\ref{fig:mysql27287}, 
there are {\color{red} XXX} functions identified 
as having super-linear complexity. 
If we simply rank reported functions using their costs, 
\texttt{main} function will always be ranked as No.1. 
\commentty{Why is \texttt{main} ranked at first? 
What is the expected root cause?}
Therefore, \emph{
more advanced techniques are needed to tell developers 
which function is the root cause from profiling results of existing techniques. 
}


Second, existing techniques incur significant runtime overheads. 
In our evaluation, we implement RMS and DSS to collect input size information. 
For most benchmarks, RMS incurs 50$X$ runtime overhead. 
DSS collects less information, and for most benchmarks, it incurs 20$X$ runtime overhead. 
\commentty{Why do they incur large overheads? What are the underlying limitations?}
\emph{Due to the extremely large overhead, 
we cannot deploy these two techniques in production-run scenario. }


Third, one option to mitigate runtime overhead is using
sampling, which is a widely-used technique to lower runtime overhead 
and deploy existing techniques to production 
environments~\cite{SongOOPSLA2014,liblit03,liblit05,CCI}. 
%
However, \emph{existing algorithmic profiling techniques are not cooperative well with
sampling approach.} 
Take MySQL\#27287 in Figure~\ref{fig:mysql27287} as an example,
the $n^2$ complexity can only be observed on the side of the outer loop. 
Each execution of the outer loop contributes almost all of 
the execution time during performance failure runs. 
If we sample the outer loop, 
we may not get any sample and thus miss the opportunity to 
identify the $n^2$ complexity, or we get a sample but have to 
record all 
information for the sampled executions of the outer loop, 
which can still  have a non-tolerable overhead. 


\commentty{It would be better to add 1-2 sentence to describe
how our approach addresses each of the three limitations.}
