
\section{Background}
\label{sec:back}

In this section, we present preliminaries of 
algorithmic profiling and discuss existing techniques and their
limitations. 

\subsection{Preliminaries}
\label{subsec:pre}

To conduct algorithmic profiling for a given code construct,
we first need to record input and cost information in multiple program runs. 
We then plot the recorded information with input size as x-axis and cost as y-axis.
In the end, we infer complexity for the code construct as a cost function of input size. 
%\commentty{To help reviewers better understand algorithmic profiling,
%we can rephrase the example of Figure 1 to illustrate algorithmic profiling and
%show how the cost function looks like.}

To conduct algorithmic profiling effectively, 
we need to define suitable metrics for input and cost. 

\noindent{\underline{Input Metric.}}
%
Inputs for a code construct can be viewed at different stack layers.
For example, from the OS layer, a code construct can take 
inputs through memory read, 
so that accessed memory cells can be used to measure inputs.
From the application layer, a code construct can take inputs 
from other components of the same 
program through internal data structures. 

Read Memory Size (RMS)~\citep{Aprof1,Aprof2}  and Data Structure Size (DSS)~\citep{AlgoProf} 
are two commonly used input metrics. 
RMS is measured as the the number of distinct memory cells 
whose first access is read. Both reads executed by the code 
construct directly and  reads executed by the callees from 
the code construct are considered. 

%Aprof \citep{Aprof1} uses Read Memory Size (RMS) as an input metric. 
%Given a dynamic instance of a code construct,
%Aprof records its RMS as the number of distinct memory cells 
%whose first access is read. 
%Both reads executed by the code construct directly and 
%reads executed by the callees from the code construct are considered. 
%In their follow-up work~\cite{Aprof2}, 
%RMS is extended  to handle inputs from library calls 
%and from different threads. 

Take MySQL\#27287 (Figure~\ref{fig:mysql27287}) as an example, 
during an execution of the buggy loop, the input size
computed by RMS is approximately 
two times the number of actual accessed elements inside array \texttt{items}. 
This is because different \texttt{XML\_NODE}s' \texttt{level} and \texttt{type} 
fields are read in different loop iterations. 
Although variable \texttt{p} and \texttt{level} are also read in each iteration,
RMS only considers distinct memory cells and 
only increments its value for the first read on these 
two variables in the first iteration. 
For the outer loop, which is not shown in Figure~\ref{fig:mysql27287}, 
its RMS is roughly equal to the size of \texttt{items}, 
because the outer loop invokes \texttt{xml\_parent\_tag()} for every
\texttt{XML\_NODE} in the \texttt{items} and RMS only 
counts distinct memory cells. 


Data Structure Size (DSS) is measured as
the number of distinct accessed elements of a data structure.  
%
%AlgoProf~\citep{AlgoProf} uses data structure size (DSS) 
%as input size for each dynamic instance of a code construct.
%DSS is measured as the number of distinct accessed elements of a data structure.  
In Figure~\ref{fig:mysql27287}, if we focus on array \texttt{items},
in one execution of the inner loop, the input size computed by DSS 
is equal to the number of accessed array elements. 
As for the outer loop, the input size computed by DSS  is
equal to the size of  \texttt{items}, because
it invokes \texttt{xml\_parent\_tag()} for every element in \texttt{items}.


\noindent{\underline{Cost Metric.}}
When a code construct is executed,
it could consume many different types of resources, 
such as CPU cycles, network bandwidth, storage, and so on.
In this paper, we focus on CPU cycles (or computation cost), 
since computation cost is often correlated 
with user-perceived software slowdown~\cite{SongOOPSLA2014}.
We leave the measurement for other types of 
consumed resources for future work.


Computation cost can be measured 
as the number of executed basic blocks (BBs)~\cite{Aprof1,Aprof2}.
The underlying assumption is that each executed 
BB will roughly consume the same CPU cycles. 
Executed instructions can also be used as cost metric. 
The number of executed instructions is highly correlated with executed BBs.
For a buggy loop, the number of loop iterations (LIs) can represent 
the computation cost of a dynamic loop instance.
Given a recursive function, the number of recursive invocations (RIs) 
of the function can serve as execution cost.
Similarly to BBs, if we use LIs and RIs, 
we assume that each loop iteration or recursive invocation 
roughly consumes the same computation cost. 



\subsection{Limitations in Existing Techniques}
\label{subsec:existing}

%\commentty{If space not permitted, we can merge
%this subsection with introduction.}

%
There are at least three limitations in existing algorithmic profiling techniques. 
%
First, existing techniques can only 
attribute complexity to every executed code construct, 
while cannot provide more information about 
which code construct is the root cause 
for a performance slowdown recorded by multiple 
execution profiles. 
For example, after applying Aprof~\cite{Aprof1,Aprof2} 
to the performance bug in Figure~\ref{fig:mysql27287}, 
there are 16 functions identified 
as having super-linear complexity. 
If we simply rank reported functions using their costs, 
\texttt{main} function will always be ranked as No.1. 
This is because callees' cost is aggregated to their callers, 
while code constructs involving computation in high complexity 
are buried inside call chains.
Therefore, \emph{
more advanced techniques are needed to tell developers 
which function is the root cause from profiling results of existing techniques. 
}


Second, existing techniques incur significant runtime overheads. 
They simply record all direct information to calculate the metrics, 
failing to consider programs' structures and missing opportunities 
to optimize performance.
For example, as an existing technique, Aprof can incur 
30X slowdown in previous evaluation~\cite{Aprof1,Aprof2}. 

%\commentty{Why do they incur large overheads? What are the underlying limitations?}
%\emph{Due to the extremely large overhead, 
%we cannot deploy these two techniques in production-run scenario. }



Third, one option to mitigate runtime overhead is leveraging
sampling, which is a widely-used technique to lower runtime overhead 
and deploy existing techniques to production 
environment~\cite{SongOOPSLA2014,liblit03,liblit05,CCI}. 
However, \emph{existing algorithmic profiling techniques are not cooperative well with
sampling approach.} 
Take MySQL\#27287 in Figure~\ref{fig:mysql27287} as an example,
the $n^2$ complexity can only be observed on the side of the outer loop. 
One single execution of the outer loop contributes almost all of 
the execution time during performance failure runs. 
If we try to sample the outer loop, 
we may not get any sample and thus miss the opportunity to 
identify the $n^2$ complexity, or we get a sample but have to 
record all 
information for the sampled execution of the outer loop, 
which can still  lead to a non-tolerable slowdown. 



To address the above limitations, 
we design a novel ranking mechanism to identify 
functions that contain complexity problems causing the recorded slowdown. 
We infer complexity from the side of inner loops 
for cases like MySQL\#27287, 
and successfully apply sampling to algorithmic 
profiling to lower runtime overhead.
We will discuss more detailed design 
later in Section~\ref{sec:online}. 

